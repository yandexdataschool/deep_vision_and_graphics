{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "hsbyb4tyki9nx32utdtjpk",
    "id": "71AQJg3CDMn9"
   },
   "source": [
    "# Homework - ConvNeXt and all the King's horses\n",
    "\n",
    "\n",
    "Let's check how ConvNeXt architecture looks like and which training technics were used to make it work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework assumes than you have already solved the previous homework.\n",
    "\n",
    "The homework contains three parts:\n",
    "\n",
    "1. Implementation of some stuff for ConvNeXt network definition\n",
    "2. Implementation of some other stuff for effective training\n",
    "3. Experiments with model averaging, label smoothing and layer norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prerequesites: dataset and data loaders\n",
    "\n",
    "We will continue to use Tiny-Imagenet dataset. So let's download it in a way, how we did it in the previous homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "k1eayz1ur2mqly9zrk5my",
    "id": "sCvh1ICbHNCE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!S:bash\n",
    "# if you are in colab, just add '!' in the start of the following line\n",
    "! wget --no-check-certificate 'https://raw.githubusercontent.com/yandexdataschool/deep_vision_and_graphics/fall21/homework01/tiny_img.py' -O tiny_img.py\n",
    "! wget --no-check-certificate 'https://raw.githubusercontent.com/yandexdataschool/deep_vision_and_graphics/fall21/homework01/tiny_img_dataset.py' -O tiny_img_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "5nh892g5zpl9qv5fki8vpk",
    "id": "5rQhiYyRDMoG"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "from tiny_img import download_tinyImg200\n",
    "data_path = '.'\n",
    "download_tinyImg200(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some code from the previous homework, that defines training data augmentations and represents validation data as dataset (class `TinyImagenetValDataset` below). Feel free to copy-paste the code from your solution of the previous homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "g2i37mixtk9kkxkki1y8",
    "id": "rS_-00tYDMoB"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "\n",
    "def get_computing_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_computing_device()\n",
    "print(f\"Our main computing device is '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "\n",
    "train_trainsforms = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.RandomRotation(5),\n",
    "     # YOUR CODE : copy-paste your transform parameters for color jitter from the previous homework\n",
    "     transforms.SOME_OTHER_AUGMENTATION_FOR_COLOR_JITTER\n",
    "     # you may add any other transforms here but color jitter should be enough\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "jrzsbgniodgtg1hif324k9",
    "id": "5vq5Cm0ADMoK"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "import tiny_img_dataset\n",
    "# you may use torchvision.datasets.ImageFolder() with the same parameters for loading train dataset \n",
    "train_dataset = tiny_img_dataset.TinyImagenetRAM('tiny-imagenet-200/train', transform=train_trainsforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more block from the previous homework that we will need here (fill free to copy-paste it from your previous solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class TinyImagenetValDataset(Dataset):\n",
    "    def __init__(self, root, transform=transforms.ToTensor()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = root\n",
    "        with open(os.path.join(root, 'val_annotations.txt')) as f:\n",
    "            annotations = []\n",
    "            for line in f:\n",
    "                img_name, class_label = line.split('\\t')[:2]\n",
    "                annotations.append((img_name, class_label))\n",
    "\n",
    "        # 1. define self.classes - list of sorted class labels from annotations\n",
    "        # it should look like self.classes from \"TinyImagenetRAM\"\n",
    "        # YOUR CODE\n",
    "        self.classes = ...\n",
    "        \n",
    "        assert len(self.classes) == 200, len(self.classes)\n",
    "        assert all(self.classes[i] < self.classes[i+1] for i in range(len(self.classes)-1)), 'classes should be ordered'\n",
    "        assert all(isinstance(elem, type(annotations[0][1])) for elem in self.classes), 'your just need to reuse class_labels'\n",
    "\n",
    "        # 2. self.class_to_idx - dict from class label to class index\n",
    "        self.class_to_idx = {item: index for index, item in enumerate(self.classes)}\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images, self.targets = [], []\n",
    "        for img_name, class_name in tqdm.tqdm(annotations, desc=root):\n",
    "            img_name = os.path.join(root, 'images', img_name)\n",
    "            # 3. load image and store it in self.images (your may want to use tiny_img_dataset.read_rgb_image)\n",
    "            # store the class index in self.targets\n",
    "            # YOUR CODE\n",
    "            image = ...\n",
    "            \n",
    "            assert image.shape == (64, 64, 3), image.shape\n",
    "            self.images.append(Image.fromarray(image))\n",
    "            self.targets.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # take image and its target label from \"self.images\" and \"self.targets\", \n",
    "        # transform the image using self.transform and return the transformed image and its target label\n",
    "        \n",
    "        # YOUR CODE\n",
    "        image = ...\n",
    "        image = self.transform(image)\n",
    "        target = ...\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "\n",
    "val_dataset = TinyImagenetValDataset('tiny-imagenet-200/val', transform=transforms.ToTensor())\n",
    "\n",
    "assert all(train_dataset.classes[i] == val_dataset.classes[i] for i in range(200)), \\\n",
    "    'class order in train and val datasets should be the same'\n",
    "assert all(train_dataset.class_to_idx[elem] == val_dataset.class_to_idx[elem] for elem in train_dataset.classes), \\\n",
    "    'class indices should be the same'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "6md8io0fesfby4r9per3jb",
    "id": "tY6OUeOODMoN"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "batch_size = 64\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "hsq566ut87vokpkiq68",
    "id": "HBgW-gzwDMoQ"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "val_batch_gen = torch.utils.data.DataLoader(val_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ConvNeXt architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the key differences between resnet-50 and ConvNeXt architectures?\n",
    "- stem redesign: single conv layer with kernel size 4 and stride 4 instead of combination of conv layer and max pooling (+0.1%)\n",
    "- stage ratio 1:1:3:1 or 1:1:9:1 (+0.6%)\n",
    "- depthwise separable convolutions (introduced in ResNeXt) with inverted bottlenecks (introduced in MobileNet v2) (+1.1%)\n",
    "- some micro design changes: fewer activations, fewer normalization layers, GELU and LN usage (+0.9%)\n",
    "- replacing strided convolutions in blocks on separate downscale convolutions (+0.5%)\n",
    "- improved training technics (+2.7%)\n",
    "  - Longer training (90 -> 300 epochs) \n",
    "  - AdamW optimizer, warm-up, cosine scheduler (we will discuss it in seminar 3).\n",
    "  - Advanced data augmentation techniques: Mixup, Cutmix, RandAugment, Random Erasing (we will discuss it in seminar 3)\n",
    "  - Regularization schemes: Stochastic Depth, Label Smoothing\n",
    "  - LayerScale\n",
    "  - exponential moving average on network weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><h3>Architecture</h3></td>\n",
    "        <td><h3>Improvements</h3></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"./convnext_arch.png\" alt=\"drawing\" width=\"500\"/></td>\n",
    "        <td><img src=\"./convnext_impr.png\" alt=\"drawing\" width=\"500\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 ConvNeXt block design\n",
    "\n",
    "Here is how a single block looks like\n",
    "\n",
    "<img src=\"./convnext_block.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Depthwise convolutions\n",
    "\n",
    "Depthwise convolutions is the special case of group convolutions. Here is a good illustration of how group convolution works:\n",
    "\n",
    "<img src=\"./group_conv.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "([image credit](https://cvml-expertguide.net/terms/dl/layers/convolution-layer/grouped-convolution/))\n",
    "\n",
    "Group convolution takes 4d tensor of shape [N,C,H,W] as input, splits it evenly on K groups (K is hyperparameter; K=4 in image above) along the channel dimension, applies vanilla 2d convolution to each group with their own kernels and then concat the output tensors along the channel dimension.\n",
    "\n",
    "In depthwise convolution number of groups is equal to the number of input channels C. It is usually followed by \"pointwise convolution\" - convolution with 1x1 kernel, which changes the number of output channels.\n",
    "\n",
    "<img src=\"./dw_conv.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "([image credit](https://www.researchgate.net/figure/Depthwise-separable-convolutions_fig1_358585116))\n",
    "\n",
    "The number of output channels in depthwise convolution by design is greater or equal than number of input channels (why?). For group convolutions the number of output channels could not be less than the number of groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can implement depthwise convolution 7x7 in pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_channels = 96\n",
    "n_output_channels = 192\n",
    "# TODO: examine nn.Conv2d doc and create depthwise conv layer with 96 input channels, 192 output channels, 7x7 kernel\n",
    "layer = nn.Conv2d(... , groups=...)\n",
    "\n",
    "parameters_size = sum([elem.size().numel() for elem in layer.parameters()])\n",
    "assert parameters_size == (7*7*192 + 192), parameters_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that amount of parameters for kernel is 7x7x192. Vanilla Conv2d would have 7x7x192x96 parameters in kernel under the same settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 LayerNorm and GELU\n",
    "\n",
    "You are parially familiar with LayerNorm and GELU layers from lecture 2. And we will discuss these layers on lecture 3 and seminar 3. \n",
    "\n",
    "What is important to know here is that the authors of ConvNeXt replaced ReLU with GELU and BatchNorm with LayerNorm and got some improvements (+0.1%).\n",
    "\n",
    "In the experiments section below we will check if LayerNorm really helps in our training setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we start, let's implement simple wrapper over nn.LayerNorm so than it would work with 4d tensors more like batch norm does. BatchNorm2D expects to work with [N,C,H,W] tensors and normalizes the channel dimension. nn.LayerNorm normalizes the last dimension of the input tensor.\n",
    "\n",
    "So let's permute dimensions in LayerNorm2d forward() method, apply the standard nn.LayerNorm to the new tensor and permute its dimensions back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: YOUR CODE\n",
    "        # 1. permute tensor dimensions so that channel dim became the last\n",
    "        # 2. apply self.ln\n",
    "        # 3. permute tensor dimensions back\n",
    "        x = x.permute(...)\n",
    "        x = ...\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((2, 5, 4, 3))\n",
    "layer = LayerNorm2d(5)\n",
    "out = layer(x)\n",
    "assert out.size() == x.size()\n",
    "parameters_size = sum([elem.size().numel() for elem in layer.parameters()])\n",
    "assert parameters_size == 10, parameters_size  # 5 for channel weights and 5 for biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 LayerScale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LayerScale is a technique used to force residual branch to work more like residual branch.\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><center><h4>Original residual branch for transformer</h4></center></td>\n",
    "        <td><center><h4>Residual branch with LayerScale</h4></center></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><img src=\"./layer_scale-orig_residual.png\" alt=\"drawing\" height=\"400\"/></center></td>\n",
    "        <td><center><img src=\"./layer_scale-new_residual.png\" alt=\"drawing\" height=\"400\"/></center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "LayerScale downscales residual branch output with some (learnable) weights which are very small at the begining (1e-6 in our experiments and in paper) but can become larger during training.\n",
    "\n",
    "Scaling is applied to each channel independenly. Here is implementation of the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerScale2d(nn.Module):\n",
    "    def __init__(self, dim, layer_scale_init_value):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim, 1, 1)), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE: just scale x on self.gamma\n",
    "        x = ...\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((2, 5, 4, 3))\n",
    "layer_scale_init_value = 1e-5\n",
    "layer = LayerScale2d(5, layer_scale_init_value)\n",
    "out = layer(x)\n",
    "assert out.size() == x.size()\n",
    "assert np.allclose(out.detach().numpy(), x.numpy()*layer_scale_init_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Stochastic Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic depth was introduced in [paper](https://arxiv.org/pdf/1603.09382.pdf) as a way of overfitting reduction. You can think of it as about dropout on residual branches. Block with residual connection with stochastic depth module looks like `y = x + DropPath(ResidualNet(x))` instead of classic `y = x + ResidualNet(x)`\n",
    "\n",
    "Let's implement `DropPath` module. Its only parameter is `drop_prob` - probability to zero-out its input. Don't forget to devide the result on `(1-drop_prob)` in order to fix mean value of output in train mode (as it is usually done in `Dropout` layer).\n",
    "\n",
    "(This layer will be also discussed in seminar dedicated to vision transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        # YOUR CODE: generate random tensor, binarize it, cast to x.dtype, multiply x by the mask, \n",
    "        # devide the result on keep_prob\n",
    "        random_tensor = torch.rand(...)\n",
    "        output = ...\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = DropPath(0.5)\n",
    "\n",
    "x = torch.rand((10,5,4,3))\n",
    "\n",
    "layer.eval()\n",
    "out = layer(x)\n",
    "assert out.size() == x.size()\n",
    "assert (out == x).all()\n",
    "\n",
    "layer.train()\n",
    "out = layer(x)\n",
    "assert out.size() == x.size()\n",
    "dropped_samples_mask = torch.isclose(out, torch.zeros([1])).all(dim=(1,2,3))\n",
    "n_dropped_samples = dropped_samples_mask.to(float).sum()\n",
    "assert n_dropped_samples > 2 and n_dropped_samples < 8, n_dropped_samples\n",
    "\n",
    "layer = DropPath(0.1)\n",
    "out = layer(x)\n",
    "dropped_samples_mask = torch.isclose(out, torch.zeros([1])).all(dim=(1,2,3))\n",
    "scaled_samples_mask = torch.isclose(out, x/0.9).all(dim=(1,2,3))\n",
    "assert torch.logical_or(dropped_samples_mask, scaled_samples_mask).all()                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, image of ConvNeXt block\n",
    "\n",
    "<img src=\"./convnext_block.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "The only thing that is not shown on the image is `DropPath` block which should be applied in the end of residual branch just after `LayerScale2d`\n",
    "\n",
    "We will create `ConvNextBlock` that can operate with `LayerNorm2d` and `BatchNorm2d` depending on parameter `use_bn` and will check what will perform better in our experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    def __init__(self, dim, drop_rate=0., layer_scale_init_value=1e-6, use_bn=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE: define self.depthwise_conv  self.pointwise_conv1 self.pointwise_conv2\n",
    "        self.depthwise_conv = nn.Conv2d(...)  # depthwise conv 5x5, padding 2, dim->dim\n",
    "        \n",
    "        self.norm = LayerNorm2d(dim) if not use_bn else nn.BatchNorm2d(dim)\n",
    "        \n",
    "        self.pointwise_conv1 = nn.Conv2d(...)  # 1x1 conv, dim -> dim*4  YOUR CODE\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "        self.pointwise_conv2 = nn.Conv2d(...)  # 1x1 conv, 4*dim -> dim YOUR CODE\n",
    "\n",
    "        self.layer_scale = LayerScale2d(dim, layer_scale_init_value) if layer_scale_init_value > 0 else nn.Identity()\n",
    "        self.drop_path = DropPath(drop_rate) if drop_rate is not None and drop_rate > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        # YOUR CODE: sequentially apply to x: depthwise_conv + norm + pointwise_conv1 + activation + pointwise_conv2 + layer_scale\n",
    "        x = ...\n",
    "        \n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_w_ln = ConvNextBlock(7, 0.1, 1e-6, use_bn=False)\n",
    "\n",
    "x = torch.rand([2,7,4,3])\n",
    "out = block_w_ln(x)\n",
    "\n",
    "assert out.size() == x.size()\n",
    "n_dwconv_parameters = sum([elem.size().numel() for elem in block_w_ln.depthwise_conv.parameters()])\n",
    "assert n_dwconv_parameters == 5*5*7 + 7\n",
    "n_pwconv1_parameters = sum([elem.size().numel() for elem in block_w_ln.pointwise_conv1.parameters()])\n",
    "assert n_pwconv1_parameters == 7*7*4 + 7*4\n",
    "n_pwconv2_parameters = sum([elem.size().numel() for elem in block_w_ln.pointwise_conv2.parameters()])\n",
    "assert n_pwconv2_parameters == 7*7*4 + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_w_bn = ConvNextBlock(7, 0.1, 1e-6, use_bn=True)\n",
    "\n",
    "x = torch.rand([2,7,4,3])\n",
    "out = block_w_bn(x)\n",
    "assert out.size() == x.size()\n",
    "\n",
    "n_block1_parameters = sum([elem.size().numel() for elem in block_w_ln.parameters()])\n",
    "n_block2_parameters = sum([elem.size().numel() for elem in block_w_bn.parameters()])\n",
    "assert n_block1_parameters == n_block2_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ConvNeXt-like arch for Tiny-Imagenet\n",
    "\n",
    "Since images in tiny-imagenet are 4x downsampled version of imagenet images, we are going to design our own network configuration by reducing: 1) amount of layers; 2) amount of neurons in layers; 3) amount of downsampling layers which downsample feature maps.\n",
    "\n",
    "Here is our network config:\n",
    "1. Stem: Conv 2x2 with stride 2, output features = 32 (instead of conv 4x4 with stride 4)\n",
    "2. Three block levels:\n",
    "   - [dwise conv5x5, ch=32; conv1x1, ch=128; conv1x1, ch=32] x 2 + downsampling block\n",
    "   - [dwise conv5x5, ch=64; conv1x1, ch=256; conv1x1, ch=64] x 2 + downsampling block\n",
    "   - [dwise conv5x5, ch=128; conv1x1, ch=512; conv1x1, ch=128] x 2\n",
    "3. Global average pooling + FC(200) + softmax\n",
    "\n",
    "Network config is inspired by the network configs used in the previous homework. It's rather designed to provide fast training and experiments conducting in the current homework and doesn't pretend on being optimal choice w.r.t accuracy/speed trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "f985tf2dvssqwmyc6w99d",
    "id": "u_mbfRXMDMob"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "class GlobalAveragePool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stem(out_channels, use_bn):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=...), # YOUR CODE; conv 2x2, stride 2, padding 0\n",
    "        nn.BatchNorm2d(out_channels) if use_bn else LayerNorm2d(out_channels)\n",
    "    )\n",
    "\n",
    "def create_downscale_block(in_channels, out_channels, use_bn):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels) if use_bn else LayerNorm2d(in_channels),\n",
    "        nn.Conv2d(...)  # YOUR CODE: conv 2x2, stride 2, padding 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convnext_like_network(config=None, use_bn=False, drop_rate=None):\n",
    "    \"\"\"\n",
    "    Creates ConvNeXt like network according to config\n",
    "    \"\"\"\n",
    "    model = nn.Sequential()\n",
    "\n",
    "    default_config = [[32, 32], [64, 64], [128, 128]]\n",
    "    config = config or default_config\n",
    "\n",
    "    stem_out_channels = config[0][0]\n",
    "    # YOUR CODE: create stem\n",
    "    model.add_module('stem', create_stem(...))\n",
    "\n",
    "    # progressivily increase drop rate from 0 to 'drop_rate' \n",
    "    drop_rates = np.linspace(0, drop_rate, sum([len(e) for e in config])) if drop_rate is not None else None\n",
    "    \n",
    "    layer_index = 0\n",
    "    for block_index in range(len(config)):\n",
    "        for layer_index_in_block in range(len(config[block_index])):\n",
    "            out_channels = config[block_index][layer_index_in_block]\n",
    "            layer_drop_rate = drop_rates[layer_index] if drop_rates is not None else None\n",
    "\n",
    "            # YOUR CODE: add ConvNextBlock\n",
    "            model.add_module(f\"{block_index}_{layer_index_in_block}\", ConvNextBlock(...))\n",
    "            layer_index += 1\n",
    "            \n",
    "        if block_index != len(config) - 1:\n",
    "            downscale_in_channels = out_channels\n",
    "            downscale_out_channels = config[block_index+1][0]\n",
    "            # YOUR CODE: add downscale block\n",
    "            model.add_module(f'downscale_{block_index}', create_downscale_block(...))\n",
    "            \n",
    "    model.add_module('pool', GlobalAveragePool(dim=(2,3)))\n",
    "    model.add_module('norm_final', nn.BatchNorm1d(out_channels) if use_bn else nn.LayerNorm(out_channels))\n",
    "    model.add_module('logits', nn.Linear(out_channels, 200))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training technics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Label smooting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label smoothing is a regularization thechnique that slightly changes gt distributions of classes allowing positive class gt probability be less than 1.0. In fact it introduces one hyperparameter `label_smoothing` in cross-entropy and makes positive class gt prob equal to `1.0 - label_smoothing + label_smoothing/n_classes` and negative classes gt prob equal to `label_smooting / n_classes`.\n",
    "\n",
    "It helps with optimization on datasets with complex intra-class dependencies when it's hard to zero-out all negative classes probabilities. You can imagine a classic imagenet-like situation, when a neural network should distinguish several dozens of dog breeds. It's a hard task for human and in practice we don't need probabilities for all negative classes to be zeroed-out. What we need is any network prediction when the gt class probability is larger than all other class probabilities.  Introducing small positive probability on negative classes helps neural network to work better in these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "3y7p7o6s7vecpf3kpktj8v",
    "id": "cGEhRWMYDMof"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def compute_loss(predictions, gt, label_smoothing=0.0):\n",
    "    return F.cross_entropy(predictions, gt, label_smoothing=label_smoothing).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 EMA on network weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TL;DR Just check the documentation from pytorch about what what weight averaging is and how it can helps you: https://pytorch.org/docs/stable/optim.html#weight-averaging-swa-and-ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_averaged_model(model, decay=0.999):\n",
    "    # YOUR CODE: create AveragedModel instance with ema multi_avg_fn (dont forget to use 'decay' parameter)\n",
    "    averaged_model = torch.optim.swa_utils.AveragedModel(... , multi_avg_fn=...)\n",
    "\n",
    "    return averaged_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For correct processing of averaged model we will need to modify `train_model()` and `train_loop()` from the previous homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "w8rht9ygh7uns89ypozln",
    "id": "sEy0LiHxDMol",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def eval_model(model, data_generator):\n",
    "    accuracy = []\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_generator:\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            y_pred = logits.max(1)[1].data\n",
    "            accuracy.append(np.mean((y_batch.cpu() == y_pred.cpu()).numpy()))\n",
    "    return np.mean(accuracy)\n",
    "\n",
    "            \n",
    "def train_model(model, optimizer, train_data_generator, ema_model=None, label_smoothing=0.0):\n",
    "    train_loss = []\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for (X_batch, y_batch) in tqdm.tqdm(train_data_generator):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # YOUR CODE: move X_batch, y_batch to 'device', compute model outputs on X_batch, \n",
    "        X_batch = ...\n",
    "        y_batch = ...\n",
    "        predictions = ...\n",
    "\n",
    "        loss = compute_loss(predictions, y_batch, label_smoothing)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ema_model is not None:\n",
    "            # YOUR CODE: update parameters of ema model here (see pytorch doc on AveragedModel)\n",
    "            ema_model.update_parameters(...)\n",
    "\n",
    "        # metrics\n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def get_input_for_bn_recompute(data_generator):\n",
    "    for i, (x, y) in enumerate(data_generator):\n",
    "        x = x.to(device)\n",
    "        yield x\n",
    "        if i == 100:\n",
    "            break\n",
    "\n",
    "\n",
    "def train_loop(model, optimizer, train_data_generator, val_data_generator, num_epochs, ema_model=None, label_smoothing=0.0):\n",
    "    \"\"\"\n",
    "    num_epochs - total amount of full passes over training data\n",
    "    \"\"\"\n",
    "    train_metrics = defaultdict(list)\n",
    "    val_metrics = defaultdict(list)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_model(model, optimizer, train_data_generator, ema_model, label_smoothing)\n",
    "\n",
    "        if ema_model is not None:\n",
    "            # YOUR CODE: update batchnorm statistics for ema_model (see pytorch doc on AveragedModel)\n",
    "            torch.optim.swa_utils.update_bn(...) # you may need get_input_for_bn_recompute() function here\n",
    "            \n",
    "        val_accuracy = eval_model(model, val_data_generator)\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(val_accuracy * 100))\n",
    "        train_metrics['loss'].append(train_loss)\n",
    "        val_metrics['accuracy'].append(val_accuracy)\n",
    "\n",
    "        if ema_model:\n",
    "            print(\"  validation accuracy(ema): \\t\\t\\t{:.2f} %\".format(val_accuracy_ema_model * 100))\n",
    "            val_metrics['ema_model_accuracy'].append(val_accuracy_ema_model)\n",
    "            \n",
    "    print('Best model accuracy: ', max(val_metrics['accuracy']))\n",
    "    if ema_model:\n",
    "        print('Best ema model accuracy: ', max(val_metrics['ema_model_accuracy']))\n",
    "\n",
    "    plt.plot(val_metrics['accuracy'], label='model')\n",
    "    if ema_model:\n",
    "        plt.plot(val_metrics['ema_model_accuracy'], label='ema_model')\n",
    "    plt.grid()\n",
    "    plt.legend(loc='best')\n",
    "    return train_metrics, val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the preparation is done, time to run the training and check the improvements from all the stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 EMA on network weights\n",
    "\n",
    "Our baseline is ConvNeXt architecture without DropPath and LabelSmooting and with BatchNorm instead of LayerNorm.\n",
    "\n",
    "Let's check how much improvement we can get from network weights averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_convnext_like_network(use_bn=True)\n",
    "model = model.to(device)\n",
    "ema_model = create_averaged_model(model)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_metrics_ema_bn, val_metrics_ema_bn = train_loop(model, opt, train_batch_gen, val_batch_gen, num_epochs=30, ema_model=ema_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally baseline should give you >42% accuracy here and ema model should give you extra >2%. If it does not, check data augmentation and all the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Stochastic depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_convnext_like_network(use_bn=True, drop_rate=0.1)\n",
    "model = model.to(device)\n",
    "ema_model = create_averaged_model(model)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_metrics_droppath, val_metrics_droppath = train_loop(model, opt, train_batch_gen, val_batch_gen, num_epochs=30, ema_model=ema_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was implemented correctly, stochastic depth should give you +1-2% accuracy here comparing to the previous experiment (for single model and for averaged version of this model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 LabelSmooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_convnext_like_network(use_bn=True, drop_rate=0.1)\n",
    "model = model.to(device)\n",
    "ema_model = create_averaged_model(model)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_metrics_ls, val_metrics_ls = train_loop(\n",
    "    model, opt, train_batch_gen, val_batch_gen, num_epochs=30, ema_model=ema_model, label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label smoothing should slightly improve quality on ~0.1-0.5%. It should be easier to see on plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_metrics_ema_bn['accuracy'], label='baseline')\n",
    "plt.plot(val_metrics_ema_bn['ema_model_accuracy'], label='ema_baseline')\n",
    "plt.plot(val_metrics_droppath['accuracy'], label='droppath')\n",
    "plt.plot(val_metrics_droppath['ema_model_accuracy'], label='ema_droppath')\n",
    "plt.plot(val_metrics_ls['accuracy'], label='label smoothing')\n",
    "plt.plot(val_metrics_ls['ema_model_accuracy'], label='ema label smoothing')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.ylim([0.35, 0.55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 LayerNorm vs BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_convnext_like_network(use_bn=False, drop_rate=0.1)\n",
    "model = model.to(device)\n",
    "ema_model = create_averaged_model(model)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_metrics_ln, val_metrics_ln = train_loop(model, opt, train_batch_gen, val_batch_gen, num_epochs=30, ema_model=ema_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_metrics_ema_bn['accuracy'], label='baseline')\n",
    "plt.plot(val_metrics_ema_bn['ema_model_accuracy'], label='ema_baseline')\n",
    "plt.plot(val_metrics_droppath['accuracy'], label='droppath')\n",
    "plt.plot(val_metrics_droppath['ema_model_accuracy'], label='ema_droppath')\n",
    "plt.plot(val_metrics_ls['accuracy'], label='label smoothing')\n",
    "plt.plot(val_metrics_ls['ema_model_accuracy'], label='ema label smoothing')\n",
    "plt.plot(val_metrics_ln['accuracy'], label='layer norm')\n",
    "plt.plot(val_metrics_ln['ema_model_accuracy'], label='ema layer norm')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.ylim([0.35, 0.55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With layer norm more likely you will see here a drop in accuracy on ~2% (or even more) comparing to the second experiment. But it doesn't mean that ConvNeXt authors were wrong on the replacing of batch norm with layer norm. Note that their training settings were different (batch size = 4k, multi-host training, larger dataset and model, longer training etc; see appendix A in the paper). There is more like an example that batch norm is still be usefull in 2020s in some tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "- We didn't used advanced augmentations (cutmix, randaugment etc) and some good stuff in optimization (AdamW, warm-up, cosine scheduler). But we will discuss it in seminar 3. See you there!\n",
    "- We didn't check that stage ratio 1:1:3:1 really helps because we are limited by available hw for experiments. But feel free to play with architecture config if you have fast enough gpu.\n",
    "- You may check ConvNeXt v2 architecture: \"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders\" https://arxiv.org/abs/2301.00808"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- ConvNeXt architecture: \"A ConvNet for the 2020s\" https://arxiv.org/abs/2201.03545\n",
    "- ConvNeXt implementation in pytorch: https://github.com/facebookresearch/ConvNeXt/tree/main\n",
    "- LayerNorm: \"Layer Normalization\" https://arxiv.org/abs/1607.06450\n",
    "- Model averaging: \"Averaging Weights Leads to Wider Optima and Better Generalization\" https://arxiv.org/abs/1803.05407\n",
    "- Model averaging in pytorch: https://pytorch.org/docs/stable/optim.html#weight-averaging-swa-and-ema\n",
    "- LayerScale: \"Going deeper with Image Transformers\" https://openaccess.thecvf.com/content/ICCV2021/papers/Touvron_Going_Deeper_With_Image_Transformers_ICCV_2021_paper.pdf\n",
    "- Stochastic Depth: \"Deep Networks with Stochastic Depth\" https://arxiv.org/abs/1603.09382\n",
    "- Label smooting: \"Rethinking the Inception Architecture for Computer Vision\" https://arxiv.org/abs/1512.00567\n",
    "- Depth-wise separable convolutions: \"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\" https://arxiv.org/abs/1704.04861\n",
    "- Inverted bottlenacks: \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" https://arxiv.org/abs/1801.04381\n",
    "- New iteration on the architecture: \"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders\" https://arxiv.org/abs/2301.00808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "seminar_pytorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "notebookId": "0bd81ca7-4175-4905-a84c-21ed8da72299"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
