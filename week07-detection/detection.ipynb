{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "884a6408"
   },
   "source": [
    "# Seminar 7 - Object Detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b68b9da2"
   },
   "source": [
    "Today is good day to learn how to implement your own object detector!\n",
    "\n",
    "Being honest, modern object detectors are still be very complicated systems with a lot of important tricks which help neural network to perform better. So the best strategy for obj detector implementation is to take a good open-source implementation as a basis and incrementally refactor it.\n",
    "\n",
    "However all implementations have some common things in it. The goal of the current seminar is to learn that things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b69d21a4"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "502eefea"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def get_computing_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_computing_device()\n",
    "print(f\"Our main computing device is '{device}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6855a328"
   },
   "source": [
    "## 1. Pascal VOC 2012\n",
    "\n",
    "20 classes, 5717 images in train, 5823 images in validation.\n",
    "\n",
    "[[homepage]](http://host.robots.ox.ac.uk/pascal/VOC/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEX3S5tfHG2-"
   },
   "source": [
    "#### Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a99d9c92"
   },
   "outputs": [],
   "source": [
    "CLASSES = (\n",
    "        \"__background__ \",\n",
    "        \"aeroplane\",\n",
    "        \"bicycle\",\n",
    "        \"bird\",\n",
    "        \"boat\",\n",
    "        \"bottle\",\n",
    "        \"bus\",\n",
    "        \"car\",\n",
    "        \"cat\",\n",
    "        \"chair\",\n",
    "        \"cow\",\n",
    "        \"diningtable\",\n",
    "        \"dog\",\n",
    "        \"horse\",\n",
    "        \"motorbike\",\n",
    "        \"person\",\n",
    "        \"pottedplant\",\n",
    "        \"sheep\",\n",
    "        \"sofa\",\n",
    "        \"train\",\n",
    "        \"tvmonitor\",\n",
    "    )\n",
    "LABEL_TO_CLASS_INDEX = {k:i for i,k in enumerate(CLASSES)}\n",
    "CLASS_INDEX_TO_LABEL = {i:k for i,k in enumerate(CLASSES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43d5d07b"
   },
   "outputs": [],
   "source": [
    "means = np.array([0.45704785, 0.43824798, 0.40617362], dtype=np.float32)\n",
    "stds = np.array([0.23908612, 0.23509602, 0.2397311 ], dtype=np.float32)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "def transform_targets(targets):\n",
    "    targets = targets['annotation']\n",
    "    objects = targets['object']\n",
    "    classes = [LABEL_TO_CLASS_INDEX[elem['name']] for elem in objects]\n",
    "    objects = [[int(elem['bndbox']['xmin']), int(elem['bndbox']['ymin']), \n",
    "                int(elem['bndbox']['xmax']), int(elem['bndbox']['ymax'])] for elem in objects]\n",
    "    sizes = [(elem[2] - elem[0]) * (elem[3] - elem[1]) for elem in objects]\n",
    "    order = np.argsort(sizes)[::-1]\n",
    "    objects = np.array(objects, np.int32)[order]\n",
    "    classes = np.array(classes, np.int32)[order]\n",
    "    return {'xyxy': torch.from_numpy(objects), \n",
    "            'classes': torch.from_numpy(classes)}\n",
    "\n",
    "MIN_HEIGHT = 512\n",
    "MIN_WIDTH = 512\n",
    "def transforms_train(images, targets):\n",
    "    targets = transform_targets(targets)\n",
    "    images = transform_train(images)\n",
    "    images = pad_to_size(images, MIN_HEIGHT, MIN_WIDTH)\n",
    "    return images, targets\n",
    "\n",
    "def pad_to_size(images, min_height, min_width):\n",
    "    if images.shape[1] < min_height:\n",
    "        images = torchvision.transforms.functional.pad(images, (0,0,0,min_height-images.shape[1]))\n",
    "    if images.shape[2] < min_width:\n",
    "        images = torchvision.transforms.functional.pad(images, (0,0, min_width - images.shape[2], 0))\n",
    "    return images\n",
    "        \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "def transforms_test(images, targets):\n",
    "    targets = transform_targets(targets)\n",
    "    images = transform_test(images)\n",
    "    images = pad_to_size(images, MIN_HEIGHT, MIN_WIDTH)\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jscSZS2vG7hd"
   },
   "source": [
    "#### Dataset download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zwP5th0GQq2",
    "outputId": "04922ea7-e551-47a1-edd2-e3952e0c5667"
   },
   "outputs": [],
   "source": [
    "! pip install yandexdirect\n",
    "! curl -L $(yadisk-direct https://disk.yandex.com/d/ggmx3PgAITrRMA) -O voc2012.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-31TPRWuGWlH"
   },
   "outputs": [],
   "source": [
    "! unzip -qq voc2012.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TJbD5frIGv-Z"
   },
   "outputs": [],
   "source": [
    "! mkdir VOCdevkit\n",
    "! mv VOC2012 ./VOCdevkit/VOC2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdMG3bKtG-OE"
   },
   "source": [
    "#### Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "940bad62"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import VOCDetection\n",
    "\n",
    "# should work like that, but doesn't now due to server error\n",
    "# train_loader = VOCDetection(root=\"./voc_data/\", download=True, image_set='train', transforms=transforms_train)\n",
    "\n",
    "train_loader = VOCDetection(root=\"./\", download=False, image_set='train', transforms=transforms_train)\n",
    "\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_loader, \n",
    "                                              batch_size=1,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=16)\n",
    "\n",
    "# same: should work like that, but doesn't now due to server error\n",
    "# val_loader = VOCDetection(root=\"./voc_data/\", download=True, image_set='val', transforms=transforms_test)\n",
    "\n",
    "val_loader = VOCDetection(root=\"./\", download=False, image_set='val', transforms=transforms_test)\n",
    "\n",
    "val_batch_gen = torch.utils.data.DataLoader(val_loader, \n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e70e759",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_images = 6\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.tight_layout()\n",
    "for i, (x,y) in enumerate(val_batch_gen):\n",
    "    if i == n_images:\n",
    "        break\n",
    "    plt.subplot(3, n_images // 3, i+1)\n",
    "    img = x[0]*stds.reshape(3,1,1) + means.reshape(3,1,1)\n",
    "    img = img*255\n",
    "    img = img.type(torch.uint8)\n",
    "    labels = [CLASS_INDEX_TO_LABEL[ind.numpy().tolist()] for ind in y['classes'][0]]\n",
    "    img = torchvision.utils.draw_bounding_boxes(img, y['xyxy'][0], width=2, labels=labels)\n",
    "    plt.imshow(img.numpy().transpose(1,2,0))\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f55643d1"
   },
   "source": [
    "## 2. FCOS - Fully Convolutional One-Stage Object Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7395943"
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1904.01355v5.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b61c29e"
   },
   "source": [
    "### 2.1 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ecae528"
   },
   "source": [
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-23_at_3.34.09_PM_SAg1OBo.png\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4e0f1d4"
   },
   "source": [
    "Let's implement our detector on the top of resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1673e827"
   },
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18685bed",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c62d987"
   },
   "outputs": [],
   "source": [
    "N_SCALES = 5\n",
    "N_HEAD_LAYERS = 4\n",
    "\n",
    "    \n",
    "\n",
    "class RegressionHead(torch.nn.Module):\n",
    "    def __init__(self, n_head_layers, n_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE: sequential with n_head_layers Conv2d(3x3 with padding) + relu\n",
    "        self.layers = torch.nn.Sequential(...)\n",
    "        \n",
    "        self.final_conv = torch.nn.Conv2d(n_features, 4, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class ClassificationHead(torch.nn.Module):\n",
    "    def __init__(self, n_head_layers, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        # YOUR CODE: same story, sequential but with two final heads\n",
    "        self.layers = torch.nn.Sequential(...)\n",
    "            \n",
    "        self.cls_pred = torch.nn.Conv2d(n_features, n_classes, kernel_size=1)\n",
    "        self.centerness_pred = torch.nn.Conv2d(n_features, 1, kernel_size=1)\n",
    "        \n",
    "        # YOUR CODE: init self.cls_pred.bias with constant b so that sigmoid(b) = 0.01\n",
    "        init_prob = 0.01  # for focal loss\n",
    "        \n",
    "        torch.nn.init.constant_(self.cls_pred.bias, ...)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        cls = self.cls_pred(x)\n",
    "        centerness = self.centerness_pred(x)\n",
    "        return cls, centerness\n",
    "\n",
    "    \n",
    "class FCOS(torch.nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "        self.resnet18.train(False)\n",
    "        n_features = 256\n",
    "        \n",
    "        self.conv_p5 = torch.nn.Conv2d(512, n_features, kernel_size=(1,1))\n",
    "        self.conv_p6 = torch.nn.Conv2d(n_features, n_features, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv_p7 = torch.nn.Conv2d(n_features, n_features, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv_p4 = torch.nn.Conv2d(256, n_features, kernel_size=(1,1))\n",
    "        self.conv_p3 = torch.nn.Conv2d(128, n_features, kernel_size=(1,1))\n",
    "        \n",
    "        self.refinement_conv_p3 = torch.nn.Conv2d(n_features, n_features, kernel_size=(1,1))\n",
    "        self.refinement_conv_p4 = torch.nn.Conv2d(n_features, n_features, kernel_size=(1,1))\n",
    "        self.refinement_conv_p5 = torch.nn.Conv2d(n_features, n_features, kernel_size=(1,1))\n",
    "        self.refinement_conv_p6 = torch.nn.Conv2d(n_features, n_features, kernel_size=(1,1))\n",
    "        self.refinement_conv_p7 = torch.nn.Conv2d(n_features, n_features, kernel_size=(1,1))\n",
    "        self.refinement_convs = [self.refinement_conv_p3, self.refinement_conv_p4, self.refinement_conv_p5, \n",
    "                                 self.refinement_conv_p6, self.refinement_conv_p7]\n",
    "                                \n",
    "        self.cls_head = ClassificationHead(N_HEAD_LAYERS, n_features, n_classes)\n",
    "        self.reg_head = RegressionHead(N_HEAD_LAYERS, n_features)\n",
    "        \n",
    "        self.regression_scales = torch.nn.Parameter(torch.ones(len(self.refinement_convs), dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.resnet18.conv1(x)\n",
    "        x = self.resnet18.bn1(x)\n",
    "        x = self.resnet18.relu(x)\n",
    "        x = self.resnet18.maxpool(x)\n",
    "\n",
    "        c2 = self.resnet18.layer1(x)\n",
    "        c3 = self.resnet18.layer2(c2)\n",
    "        c4 = self.resnet18.layer3(c3)\n",
    "        c5 = self.resnet18.layer4(c4)\n",
    "        # don't compute avgpool+flatten+fc in resnet\n",
    "        \n",
    "        # YOUR CODE\n",
    "        p5 = self.conv_p5(c5)\n",
    "        p6 = ...\n",
    "        p7 = ...\n",
    "        p4 = self.conv_p4(c4) + F.interpolate(p5, size=c4.shape[2:], mode='nearest')\n",
    "        p3 = ...\n",
    "        \n",
    "        cls_results = []\n",
    "        reg_results = []\n",
    "        centerness_results = []\n",
    "        for i, scale_features in enumerate([p3, p4, p5, p6, p7]):\n",
    "            # YOUR CODE apply refinement_convs and heads\n",
    "            refined_features = ...\n",
    "            cls, centerness = ...\n",
    "            reg = self.reg_head(refined_features) * self.regression_scales[i]\n",
    "            cls_results.append(cls)\n",
    "            reg_results.append(reg)\n",
    "            centerness_results.append(centerness)\n",
    "        return cls_results, reg_results, centerness_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0133587"
   },
   "outputs": [],
   "source": [
    "detector = FCOS(n_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cddab362"
   },
   "outputs": [],
   "source": [
    "pred = detector(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0401dcb1"
   },
   "source": [
    "### 2.2 Assignment and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8c4183a"
   },
   "source": [
    "In FCOS in each pixel we predict box class label, distances to left, right, top and bottom borders of box object and centerness heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "935f2ce6"
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/225/1*8Ou4qNoKscReoOzMQJ1ZYA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4ec7cd5"
   },
   "source": [
    "It all starts with assignment of gt boxes to the pixels of sourse images. \n",
    "\n",
    "Here is a quote from the paper about assignment procedure:\n",
    "\n",
    "> More specifically, we firstly compute the regression targets $(l^∗, t^∗, r^∗, b^∗)$ for each location on all feature levels.  \n",
    "> Next,  if a location satisfies $\\max(l^∗, t^∗, r^∗, b^∗) > m_i$ or $\\max(l^∗, t^∗, r^∗, b^∗) < m_{i−1}$, it is set as a negative sample and is thus not required to regress a bounding box anymore.  \n",
    "> Here $m_i$ is the maximum distance that feature level $i$ needs to regress. In this work $m_2$,$m_3$,$m_4$,$m_5$,$m_6$ and $m_7$ are set as 0, 64, 128, 256, 512 and $\\inf$, respectively. \n",
    "> Since objects with different sizes are assigned to different feature levels and most overlapping happens between objects with considerably different sizes. \n",
    "\n",
    "> If a location is still assigned to more than one ground truth boxes, we simply choose the ground truth box with minimal area as its target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e780ad0e"
   },
   "source": [
    "Once assignment is done, we compute regression and centerness targets for pixels assigned to some box. Regression targets definition is straight forward: for each pixel it's the distance from the pixel to boundaries.\n",
    "\n",
    "Centerness is defined by specific formula:\n",
    "$$\\text{centerness} = \\sqrt{\\frac{\\min(l^*, r^*)}{\\max(l^*,r^*)} \\times \\frac{\\min(t^*, b^*)}{\\max(t^*,b^*)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b2c6f66"
   },
   "outputs": [],
   "source": [
    "def calc_centerness(scale_regression):\n",
    "    \"\"\"\n",
    "    scale_regression - tensor of shape [bs, n_boxes, 4] in form of xyxy\n",
    "    \"\"\"\n",
    "    # YOUR CODE don't forget to fix div by zero\n",
    "    scale_centerness = torch.sqrt(...)\n",
    "    return scale_centerness\n",
    "    \n",
    "    \n",
    "def calc_regression_targets(object_xyxy, object_xyxy_at_scale, scale_stride):\n",
    "    x_min, y_min, x_max, y_max = torch.unbind(object_xyxy_at_scale)\n",
    "    center_points = torch.meshgrid(torch.arange(y_min, y_max), torch.arange(x_min, x_max))[::-1]\n",
    "    center_points = torch.stack(center_points, axis=-1)\n",
    "    center_points = center_points.type(torch.float32) * scale_stride + scale_stride / 2\n",
    "    \n",
    "    # YOUR CODE \n",
    "    left_top_shift = ...\n",
    "    right_bottom_shift = ...\n",
    "    result = torch.cat((left_top_shift, right_bottom_shift), axis=-1)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def get_heads_gt(targets, shapes, strides):\n",
    "    n = len(targets['classes'][0])\n",
    "    gt_classes = []\n",
    "    gt_centerness = []\n",
    "    gt_regression = []\n",
    "    n_scales = len(shapes)\n",
    "    REGRESSION_BOUNDS = [0, 64, 128, 256, 512, 1e5]\n",
    "    \n",
    "    for i in range(n_scales):\n",
    "        h, w = shapes[i][2], shapes[i][3]\n",
    "        scale_stride = strides[i]\n",
    "        \n",
    "        scale_classes = torch.zeros([h, w], dtype=torch.int32)\n",
    "        scale_regression = torch.zeros([h,w,4], dtype=torch.float32)\n",
    "        \n",
    "        for j in range(n):\n",
    "            object_xyxy = targets['xyxy'][0, j]\n",
    "            object_xyxy_at_scale = torch.round(object_xyxy / scale_stride).type(torch.int32)\n",
    "            x_min, y_min, x_max, y_max = torch.unbind(object_xyxy_at_scale)\n",
    "            if x_min >= x_max or y_min >= y_max:\n",
    "                continue\n",
    "            regression_targets = calc_regression_targets(object_xyxy, object_xyxy_at_scale, scale_stride)\n",
    "            max_regression_targets = regression_targets.max(axis=-1)[0]\n",
    "            mask = torch.logical_and(max_regression_targets >= REGRESSION_BOUNDS[i],\n",
    "                                     max_regression_targets <= REGRESSION_BOUNDS[i+1])\n",
    "            #mask = torch.ones_like(mask)\n",
    "            scale_classes[y_min:y_max, x_min:x_max] = torch.where(\n",
    "                mask, targets['classes'][0, j], scale_classes[y_min:y_max, x_min:x_max])\n",
    "            scale_regression[y_min:y_max, x_min:x_max] = torch.where(\n",
    "                mask[:,:,np.newaxis], regression_targets, scale_regression[y_min:y_max, x_min:x_max])\n",
    "            \n",
    "        scale_centerness = calc_centerness(scale_regression)\n",
    "        gt_classes.append(scale_classes[np.newaxis, np.newaxis])\n",
    "        gt_centerness.append(scale_centerness[np.newaxis, np.newaxis])\n",
    "        gt_regression.append(scale_regression.permute(2,0,1)[np.newaxis])\n",
    "    return gt_classes, gt_regression, gt_centerness\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "782ac611"
   },
   "outputs": [],
   "source": [
    "gt = get_heads_gt(y, [elem.shape for elem in pred[0]], [2**i for i in range(3, 7+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59328d46"
   },
   "source": [
    "Let's visualize the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50d3e1ff"
   },
   "outputs": [],
   "source": [
    "def get_img(x):\n",
    "    return x.numpy().transpose(1,2,0)*stds + means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a970fef"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.tight_layout()\n",
    "n_scales = len(pred[0])\n",
    "plt.subplot(6,1+n_scales,1)\n",
    "plt.imshow(get_img(x[0]))\n",
    "for i in range(n_scales):\n",
    "    # classes\n",
    "    plt.subplot(6,1+n_scales, i+2)\n",
    "    plt.imshow(gt[0][i][0, 0].numpy())\n",
    "    # centerness\n",
    "    plt.subplot(6,1+n_scales, (1+n_scales) + i+2)\n",
    "    plt.imshow(gt[2][i][0, 0].numpy())\n",
    "    # regressions\n",
    "    for j in range(4):\n",
    "        plt.subplot(6,1+n_scales, (1+n_scales)*(2+j) + i+2)\n",
    "        plt.imshow(gt[1][i][0, j].numpy())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3dc4bdd"
   },
   "outputs": [],
   "source": [
    "def decode_boxes(offsets, positive_mask, stride):\n",
    "    ys, xs = torch.where(positive_mask)\n",
    "    xs = xs * stride + stride / 2\n",
    "    ys = ys * stride + stride / 2\n",
    "    offsets = offsets.permute(1,2,0)[positive_mask]\n",
    "    x_min = xs - offsets[:,0]\n",
    "    y_min = ys - offsets[:,1]\n",
    "    x_max = xs + offsets[:,2]\n",
    "    y_max = ys + offsets[:,3]\n",
    "    return torch.stack([x_min, y_min, x_max, y_max], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64af4806"
   },
   "outputs": [],
   "source": [
    "gt_cls, gt_reg, gt_centerness = gt\n",
    "\n",
    "print('gt_boxes:')\n",
    "print(y['xyxy'].numpy())\n",
    "for i in range(len(gt_cls)):\n",
    "    mask = gt_cls[i]>0\n",
    "    stride = 2**(i+3)\n",
    "    a = decode_boxes(gt_reg[i][0], mask[0,0], stride)\n",
    "    print(f'boxes from scale {i}')\n",
    "    print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "615fd629"
   },
   "source": [
    "Any ideas, what to do with multiple possible predictions on gt instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd5a3891"
   },
   "source": [
    "### 2.3 Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "654579c2"
   },
   "source": [
    "Classification loss is so called focal loss ([paper](https://arxiv.org/pdf/1708.02002.pdf)):\n",
    "\n",
    "$$\\text{FL}(p_t) = -(1-p_t)^\\gamma \\log p_t$$\n",
    "\n",
    "where $p_t = \\text{sigmoid}(x)$ for positive class and $p_t = 1- \\text{sigmoid}(x) = \\text{sigmoid}(-x)$ otherwise. \n",
    "\n",
    "For focal loss it's important to provide correct initialization of biases in the final layer that predicts logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ae3554"
   },
   "outputs": [],
   "source": [
    "gt_cls, gt_reg, gt_centerness = gt\n",
    "pred_cls, pred_reg, pred_centerness = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d53c4829"
   },
   "outputs": [],
   "source": [
    "def calc_focal_loss(pred, gt, alpha=0.25, gamma=2.):\n",
    "    \"\"\"\n",
    "    pred - tensor of shape [bs, n_classes, h, w]\n",
    "    gt - tensor of shape [bs, 1, h, w] with elements in [0... n_classes+1]\n",
    "    \"\"\"\n",
    "    bs, n_classes, h, w = pred.shape\n",
    "    one_hot = torch.zeros((bs, n_classes+1, h, w), dtype=torch.float32)\n",
    "    gt = gt.type(torch.int64)\n",
    "    one_hot.scatter_(dim=1, index=gt, src=torch.ones_like(gt, dtype=torch.float32))\n",
    "    one_hot = one_hot[:,1:]  # remove negative class\n",
    "\n",
    "    # YOUR CODE\n",
    "    p = ...\n",
    "    pt = ...\n",
    "    log_pt = ...\n",
    "\n",
    "    return torch.sum(-alpha * (1-pt)**gamma * log_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68fc4ae6"
   },
   "outputs": [],
   "source": [
    "for i in range(len(pred_cls)):\n",
    "    print(calc_focal_loss(pred_cls[i], gt_cls[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea050694"
   },
   "source": [
    "Loss for centerness is binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51a08d88"
   },
   "outputs": [],
   "source": [
    "def calc_centerness_loss(pred, gt):\n",
    "    return torch.sum(-gt*F.logsigmoid(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f6bf257"
   },
   "outputs": [],
   "source": [
    "for i in range(len(pred_cls)):\n",
    "    mask = gt_cls[i]>0\n",
    "    if torch.any(mask):\n",
    "        loss = calc_centerness_loss(pred_centerness[i][mask], gt_centerness[i][mask])\n",
    "    else:\n",
    "        loss = 0.0\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "883e58c7"
   },
   "source": [
    "Generalized IoU for regression ([paper](https://giou.stanford.edu/GIoU.pdf),  [website](https://giou.stanford.edu))\n",
    "<img src=\"https://miro.medium.com/max/1400/1*AO24IoYBsnneViLzEqrS0A.png\" style=\"width:40%\">\n",
    "\n",
    "\n",
    "Any ideas, why do we need object `C` here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcddf7e3"
   },
   "outputs": [],
   "source": [
    "def calc_giou_loss(x, y):\n",
    "    '''\n",
    "    x - [n_boxes, 4], xyxy format\n",
    "    y - [n_boxes, 4], xyxy format\n",
    "    '''\n",
    "    x_area = (x[:,2] - x[:,0]) * (x[:,3] - x[:,1])\n",
    "    y_area = (y[:,2] - y[:,0]) * (y[:,3] - y[:,1])\n",
    "\n",
    "    intersection_start = torch.maximum(x[:,:2], y[:,:2])\n",
    "    intersection_end = torch.minimum(x[:,2:], y[:,2:])\n",
    "    intersection_wh = (intersection_end - intersection_start).clip(min=0)\n",
    "    intersection_area = intersection_wh[:, 0] * intersection_wh[:,1]\n",
    "    \n",
    "    union_area = x_area + y_area - intersection_area\n",
    "    iou = intersection_area / union_area\n",
    "\n",
    "    # YOUR CODE: box for C\n",
    "    enclosing_box_start = ...\n",
    "    enclosing_box_end = ...\n",
    "    enclosing_box_wh = ...\n",
    "    enclosing_box_area = ...\n",
    "\n",
    "    giou_value = iou - (enclosing_box_area-union_area) / enclosing_box_area\n",
    "\n",
    "    giou_loss = 1. - giou_value\n",
    "    return torch.sum(giou_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d035fc4a"
   },
   "outputs": [],
   "source": [
    "for i in range(len(pred_cls)):\n",
    "    mask = gt_cls[i]>0\n",
    "    if torch.any(mask):\n",
    "        stride = 2**(i+3)\n",
    "        decoded_boxes_pred = decode_boxes(torch.exp(pred_reg[i][0]), mask[0,0], stride)\n",
    "        decoded_boxes_gt = decode_boxes(gt_reg[i][0], mask[0,0], stride)\n",
    "        loss = calc_giou_loss(decoded_boxes_pred, decoded_boxes_gt)\n",
    "    else:\n",
    "        loss = 0.0\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5461cc47"
   },
   "source": [
    "Combining all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab1ae3b9"
   },
   "outputs": [],
   "source": [
    "def compute_loss(pred, gt):\n",
    "    gt_cls, gt_reg, gt_centerness = gt\n",
    "    pred_cls, pred_reg, pred_centerness = pred\n",
    "\n",
    "    n_scales = len(pred)\n",
    "    cl_loss = 0.0\n",
    "    reg_loss = 0.0\n",
    "    cent_loss = 0.0\n",
    "    n_positive = 0\n",
    "    for i in range(n_scales):\n",
    "        cl_loss += calc_focal_loss(pred_cls[i].cpu(), gt_cls[i])\n",
    "        \n",
    "        mask = gt_cls[i]>0\n",
    "        n_positive += torch.sum(mask)\n",
    "        if torch.any(mask):\n",
    "            cent_loss += calc_centerness_loss(pred_centerness[i].cpu()[mask], gt_centerness[i][mask])\n",
    "        \n",
    "            stride = 2**(i+3)\n",
    "            decoded_boxes_pred = decode_boxes(torch.exp(pred_reg[i].cpu()[0]), mask[0,0], stride)\n",
    "            decoded_boxes_gt = decode_boxes(gt_reg[i][0], mask[0,0], stride)\n",
    "            reg_loss += calc_giou_loss(decoded_boxes_pred, decoded_boxes_gt)\n",
    "            \n",
    "    n_positive = n_positive.clip(20)\n",
    "    cl_loss = cl_loss / n_positive\n",
    "    reg_loss = reg_loss / n_positive\n",
    "    cent_loss = cent_loss / n_positive\n",
    "    total_loss = cl_loss + reg_loss + cent_loss\n",
    "    return total_loss, (cl_loss, reg_loss, cent_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd90a840"
   },
   "outputs": [],
   "source": [
    "compute_loss(pred, gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32b921fc"
   },
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "950189eb"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "detector = FCOS(n_classes=20)\n",
    "detector = detector.to(device)\n",
    "\n",
    "opt = torch.optim.Adam(detector.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcee6166"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "num_epochs = 100\n",
    "detector.train(False)   # disable batchnorms in resnet\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_comp_loss = []\n",
    "    val_comp_loss = []\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    \n",
    "    for batch_index, (X_batch, y_batch) in enumerate(tqdm.tqdm(train_batch_gen)):\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        pred = detector(X_batch)\n",
    "        gt = get_heads_gt(y_batch, [elem.shape for elem in pred[0]], [2**i for i in range(3, 7+1)])\n",
    "\n",
    "        loss, component_loss = compute_loss(pred, gt)\n",
    "        loss = loss / batch_size\n",
    "        loss.backward()\n",
    "        if batch_index % batch_size == 0:\n",
    "            opt.step()\n",
    "        #print(loss, component_loss)\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        train_comp_loss.append(np.array([elem.data.numpy() for elem in component_loss]))\n",
    "        print(y_batch, component_loss)\n",
    "        if batch_index % 250 == 0:\n",
    "            print(\"current training loss: \\t{:.6f} , \\t component loss: {}\".format(\n",
    "                  np.mean(train_loss), np.mean(np.stack(train_comp_loss), axis=0)))\n",
    "        \n",
    "    for X_batch, y_batch in val_batch_gen:\n",
    "        X_batch = X_batch.to(device)\n",
    "\n",
    "        pred = detector(X_batch)\n",
    "        gt = get_heads_gt(y_batch, [elem.shape for elem in pred[0]], [2**i for i in range(3, 7+1)])\n",
    "\n",
    "        loss, component_loss = compute_loss(pred, gt)\n",
    "        val_loss.append(loss.data.numpy())\n",
    "        val_comp_loss.append(np.array([elem.data.numpy() for elem in component_loss]))\n",
    "        \n",
    "        if len(val_comp_loss) % 100 == 0:\n",
    "            print(\"current validation loss: \\t{:.2f} , \\t componnet loss: {}\".format(\n",
    "                  np.mean(val_loss), np.mean(np.stack(val_comp_loss), axis=0)))\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f} , \\t component loss: {}\".format(\n",
    "        np.mean(train_loss), np.mean(np.stack(train_comp_loss), axis=0)))\n",
    "    print(\"  validation loss: \\t\\t\\t{:.2f} , \\t\\t componnet loss: {}\".format(\n",
    "        np.mean(val_loss), np.mean(np.stack(val_comp_loss), axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a91718c"
   },
   "source": [
    "## 4. Inference stuff\n",
    "* NMS: `torchvision.ops.nms()`, `torchvision.ops.batched_nms`\n",
    "* metric: average precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc2TmxvvQs8c"
   },
   "source": [
    "#### Inference on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4b7b761d"
   },
   "outputs": [],
   "source": [
    "n_images = 12\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.tight_layout()\n",
    "start = 0\n",
    "\n",
    "for i, (x,y) in enumerate(val_batch_gen):\n",
    "    if i == n_images:\n",
    "        break\n",
    "    x = x.to(device)\n",
    "    pred_cls, pred_reg, pred_centerness  = detector(x)\n",
    "    \n",
    "    for j in range(5):\n",
    "        \n",
    "        # TODO: add centerness\n",
    "        pred_cls[j] = pred_cls[j] > 0.0\n",
    "        pred_cls[j] = torch.sum(pred_cls[j], dim=1)\n",
    "\n",
    "        mask = pred_cls[j]>0\n",
    "        mask = mask.cpu()\n",
    "\n",
    "        stride = 2**(j+3)\n",
    "\n",
    "        if torch.any(mask):\n",
    "            decoded_boxes_pred = decode_boxes(torch.exp(pred_reg[j].cpu()[0]), mask[0], stride)\n",
    "\n",
    "            x = x.cpu()\n",
    "            plt.subplot(3, n_images // 3, i+1)\n",
    "            img = x[0]*stds.reshape(3,1,1) + means.reshape(3,1,1)\n",
    "            img = img*255\n",
    "            img = img.type(torch.uint8)\n",
    "            img = torchvision.utils.draw_bounding_boxes(img, decoded_boxes_pred, width=2)\n",
    "            plt.imshow(img.numpy().transpose(1,2,0))\n",
    "            plt.xticks([]); plt.yticks([])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
