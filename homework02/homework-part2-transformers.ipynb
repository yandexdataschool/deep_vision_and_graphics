{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Compact Convolutional Transformers\n",
    "\n",
    "This week we try to train Vision Transformer-like network for cifar10 images classification.\n",
    "\n",
    "This task is based on paper [Escaping the Big Data Paradigm with Compact Transformers](https://arxiv.org/pdf/2104.05704.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR dataset\n",
    "\n",
    "You have already met this dataset in seminar 2.\n",
    "\n",
    "* 60k images of shape 3x32x32\n",
    "* 10 different classes: planes, dogs, cats, trucks, etc.\n",
    "\n",
    "<img src=\"cifar10.jpg\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when running in colab, un-comment this\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall19/week03_convnets/cifar.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "import numpy as np\n",
    "import random\n",
    "from cifar import load_cifar10\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"cifar_data\")\n",
    "\n",
    "class_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                        'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=[12,10])\n",
    "for i in range(12):\n",
    "    plt.subplot(3,4,i+1)\n",
    "    plt.imshow(np.transpose(X_train[i],[1,2,0]))\n",
    "    plt.title(class_names[y_train[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and loss\n",
    "\n",
    "The following code is based on homework 1-pt2 and should be familiar to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "def get_computing_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_computing_device()\n",
    "print(f\"Our main computing device is '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!L\n",
    "def compute_loss(logits, y_batch):\n",
    "    return F.cross_entropy(logits, y_batch).mean().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "means = np.array((0.4914, 0.4822, 0.4465))\n",
    "stds = np.array((0.2470, 0.2435, 0.2616))\n",
    "\n",
    "transform_augment = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((32,32), scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.RandomRotation([-5, 5]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "train_loader = CIFAR10(\"./cifar_data/\", train=True, transform=transform_augment)\n",
    "\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_loader, \n",
    "                                              batch_size=50,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=16)\n",
    "val_loader = CIFAR10(\"./cifar_data/\", train=False, transform=transform_test)\n",
    "\n",
    "val_batch_gen = torch.utils.data.DataLoader(val_loader, \n",
    "                                              batch_size=50,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Multi-head attention\n",
    "\n",
    "Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement MultiHeadAttention for beggining. It's already implemented in pytorch, so we will use `nn.MultiHeadAttention` for testing of your implementation.\n",
    "\n",
    "As a reminder, (one-head) attention implements a simple formula: $\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$, where $d_k$ is size of K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.in_proj = nn.Linear(embed_dim, 3*embed_dim)  # don't change the name\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)  # don't change the name\n",
    "        \n",
    "        self.norm_coeff = self.head_dim ** 0.5\n",
    "        \n",
    "        self.attention_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        qkv - query, key and value - it should be the same tensor since we implement self-attention\n",
    "        \"\"\"\n",
    "        # YOUR CODE\n",
    "        # 1. apply self.in_proj to qkv\n",
    "        # 2. split the result of step 1 on three equal parts of size self.embed_dim: query, key, value\n",
    "        # 3. compute scaled dot-product attention for different heads in loop. \n",
    "        #    i-th head will work on query[:, :, i*head_dim: (i+1)*head_dim], \n",
    "        #       key[:, :, i*head_dim: (i+1)*head_dim] and value[:, :, i*head_dim: (i+1)*head_dim]\n",
    "        #    important: apply dropout to attention maps _after_ softmax, but _before_ multiplication by values.\n",
    "        #    it follows original implementation and corresponds to dropping out tokens.\n",
    "        # 4. concat all results\n",
    "        # 5. apply self.out_proj to the result of step 5\n",
    "\n",
    "        raise NotImplementedError\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that your implementation works like `torch.nn.MultiheadAttention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = torch.nn.MultiheadAttention(embed_dim=128, num_heads=16, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mha = CustomMultiHeadSelfAttention(embed_dim=128, num_heads=16)\n",
    "custom_mha.in_proj.weight = mha.in_proj_weight\n",
    "custom_mha.in_proj.bias = mha.in_proj_bias\n",
    "custom_mha.out_proj.weight = mha.out_proj.weight\n",
    "custom_mha.out_proj.bias = mha.out_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "for _ in range(100):\n",
    "    a = torch.rand((1, 10, 128))\n",
    "    out1 = mha(a, a, a)[0].cpu().detach().numpy()\n",
    "    out2 = custom_mha(a).cpu().detach().numpy()\n",
    "    assert np.allclose(out1, out2, atol=1e-6), f\"{out1} {out2}\"\n",
    "    \n",
    "print (\"Congratulations! It works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Step-wise learning rate with warm-up\n",
    "\n",
    "Your task is to implement class that works as `torch.optim.lr_scheduler.StepLR` but supports warm-up.\n",
    "\n",
    "First of all, examine the docstring and implementation of `StepLR` scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.lr_scheduler.StepLR??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then implement `get_lr()` method. It should work as following:\n",
    "1. If `self.last_epoch` is in `[0, self.warmup_epochs]`, scheduler is in warm-up mode and learning rates for each parameter group should lineary increase during epochs from `self.warmup_lr_init` to `self.base_lrs` (which are the original learning rates)\n",
    "\n",
    "2. Else if `self.last_epoch - self.warmup_epochs` is not divisible by `self.step_size`, return the previous learning rates which are available through `[group['lr'] for group in self.optimizer.param_groups]`\n",
    "\n",
    "3. Else if `self.last_epoch - self.warmup_epochs` is divisible by `self.step_size` and current learning rates multiplied by `self.gamma` are not less then `self.min_lr`, then multiply them and return new values.\n",
    "\n",
    "4. Otherwise return the last learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepLRWithWarmup(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, step_size, gamma=0.1, warmup_epochs=3, warmup_lr_init=1e-5, \n",
    "                 min_lr=1e-5,\n",
    "                 last_epoch=-1, verbose=False):\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.warmup_lr_init = warmup_lr_init\n",
    "        self.min_lr = min_lr\n",
    "        \n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "            \n",
    "        if (self.last_epoch == 0):\n",
    "            return [self.warmup_lr_init for _ in self.optimizer.param_groups]\n",
    "        # YOUR CODE\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what you have written.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARMUP_LR_INIT = 1e-5\n",
    "WARMUP_EPOCHS = 10\n",
    "STEP_SIZE = 10\n",
    "GAMMA = 0.1\n",
    "MIN_LR = 1e-5\n",
    "OPT_LR = 1e-3\n",
    "\n",
    "dummy_net = nn.Sequential(nn.Linear(10,10))\n",
    "dummy_opt = torch.optim.Adam(dummy_net.parameters(), lr=OPT_LR)\n",
    "\n",
    "scheduler = StepLRWithWarmup(dummy_opt, step_size=STEP_SIZE, gamma=GAMMA, \n",
    "                                      warmup_epochs=WARMUP_EPOCHS, warmup_lr_init=WARMUP_LR_INIT,\n",
    "                                      min_lr=MIN_LR)\n",
    "\n",
    "# we need to do at least one optimizer step before calling scheduler.step()\n",
    "# in order to make pytorch happy\n",
    "dummy_opt.step()  \n",
    "\n",
    "\n",
    "learning_rates = []\n",
    "for i in range(100):\n",
    "    learning_rates.append(scheduler.get_last_lr())\n",
    "    scheduler.step()\n",
    "    \n",
    "plt.plot(learning_rates)\n",
    "plt.grid()\n",
    "plt.title('Learning rates')\n",
    "    \n",
    "assert np.isclose(learning_rates[0], WARMUP_LR_INIT), \\\n",
    "    f\"LR on the first epoch should be equal to {WARMUP_LR_INIT}, actual value {learning_rates[0]}\"\n",
    "assert np.isclose(learning_rates[WARMUP_EPOCHS], OPT_LR), \\\n",
    "    f\"LR after warmup shold be equal to {OPT_LR}, actual value {learning_rates[WARMUP_EPOCHS]}\"\n",
    "assert np.isclose(learning_rates[WARMUP_EPOCHS+STEP_SIZE-1], OPT_LR), \\\n",
    "    f\"LR after warmup + (STEP_SIZE-1) steps should be equal to {OPT_LR}, \"\\\n",
    "    f\"actual value {learning_rates[WARMUP_EPOCHS+STEP_SIZE-1]}\"\n",
    "assert np.isclose(learning_rates[WARMUP_EPOCHS+STEP_SIZE], OPT_LR*GAMMA), \\\n",
    "    f\"LR after warmup + (STEP_SIZE) steps shold be equal to {OPT_LR*GAMMA}, \" \\\n",
    "    f\"actual value {learning_rates[WARMUP_EPOCHS+STEP_SIZE]}\"\n",
    "\n",
    "assert np.isclose(learning_rates[WARMUP_EPOCHS+STEP_SIZE*2-1], OPT_LR*GAMMA), \\\n",
    "    f\"LR after warmup + (2*STEP_SIZE-1) steps should be equal to {OPT_LR*GAMMA}, \"\\\n",
    "    f\"actual value {learning_rates[WARMUP_EPOCHS+STEP_SIZE*2-1]}\"\n",
    "assert np.isclose(learning_rates[WARMUP_EPOCHS+2*STEP_SIZE], OPT_LR*GAMMA**2), \\\n",
    "    f\"LR after warmup + (2*STEP_SIZE) steps shold be equal to {OPT_LR*GAMMA**2}, \" \\\n",
    "    f\"actual value {learning_rates[WARMUP_EPOCHS+2*STEP_SIZE]}\"\n",
    "\n",
    "assert np.isclose(learning_rates[WARMUP_EPOCHS+3*STEP_SIZE], OPT_LR*GAMMA**2), \\\n",
    "    f\"LR after warmup + (3*STEP_SIZE) steps shold be equal to {OPT_LR*GAMMA**2}, \" \\\n",
    "    f\"actual value {learning_rates[WARMUP_EPOCHS+3*STEP_SIZE]}\"\n",
    "\n",
    "for i in range(WARMUP_EPOCHS):\n",
    "    expected_val = WARMUP_LR_INIT + i*(OPT_LR-WARMUP_LR_INIT) / WARMUP_EPOCHS\n",
    "    actual_val = learning_rates[i]\n",
    "    assert np.isclose(actual_val, expected_val), \\\n",
    "        f\"LR should linary increase from {WARMUP_LR_INIT} to {OPT_LR} during warmup.\"\\\n",
    "        f\"Expected: {expected_val}, actual: {actual_val}, iteration={i}\"\n",
    "assert all(elem[0] >= MIN_LR for elem in learning_rates)\n",
    "print(\"All asserts were passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Compact Convolutional Transformer\n",
    "\n",
    "During seminar you learned the main components of vision transformer: Tokenizer, Transformer encoder, position embeddings. At this point, it's expected that you have solved the ipython notebook from the seminar. If you didn't, then you know what to do before starting the current task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement Compact Convolutional Transformer (CCT). It has two main changes comparing to the original ViT that was implemented during seminar. The first one is in tokenizer. Authors of CCT suggested to apply the first convolution with stride=1 allowing transformer to work with input patches that overlaps with each other. Since simple removing of stride increases the spatial resolution of the input tensor, we use MaxPool2d with desired stride in order to reduce the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerCCT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kernel_size=3, stride=1, padding=1,\n",
    "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
    "                 n_input_channels=3,\n",
    "                 n_output_channels=64,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.tokenizer_layers = nn.Sequential(\n",
    "                # YOUR CODE\n",
    "                # it should implement the following sequence of layers:\n",
    "                # Conv2d(n_input_channels, n_output_channels, kernel_size, stride, padding, bias=False) + \n",
    "                #   + ReLU + \n",
    "                #   + MaxPool(pooling_kernel_size, pooling_stride, pooling_padding)\n",
    "                nn.Conv2d(...),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(...))\n",
    "\n",
    "        self.flattener = nn.Flatten(2, 3)  # flat h,w dims into token dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.tokenizer_layers(x)\n",
    "        y = self.flattener(y)\n",
    "        y = y.transpose(-2, -1)  # swap token dim and embedding dim\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second change is that CCT uses learnable pooling instead of class token for global features extraction. Its formula is similar to attention formula:\n",
    "$$y = \\text{softmax}(WX^T+b) * X$$\n",
    "where $X$ - layer input - matrix of shape [batch_size, n_tokens, n_embedding], $W$, $b$ - learnable parameters, that transform each token embedding vector to 1 element (in fact it's just a linear layer with output_dim=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqPooling(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.attention_pool = nn.Linear(embedding_dim, 1)\n",
    "          \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE\n",
    "        # 1. apply self.attention_pool to x\n",
    "        w = ...\n",
    "        # 2. take softmax over the first dim (token dim)\n",
    "        w = F.softmax(w, dim=1)\n",
    "        # 3. transpose two last dims of w to make its shape be equal to [N, 1, n_tokens]\n",
    "        w = ...\n",
    "        \n",
    "        # 4. call torch.matmul from 'w' and input tensor 'x'\n",
    "        y = ...\n",
    "        \n",
    "        # 5. now 'y' shape is [N, 1, embedding_dim]. Squeeze the second dim\n",
    "        y = ...\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three modules were implemented by you during seminar, so you can just copy-paste their implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(embedding_dim, mlp_size, dropout_rate):\n",
    "    return nn.Sequential(\n",
    "        # YOUR CODE: Linear + GELU + Dropout + Linear + Dropout\n",
    "        nn.Linear(...)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        # YOUR CODE: \n",
    "        # generate random tensor, binarize it, cast to x.dtype, multiply x by the mask, divide the result on keep_prob\n",
    "        random_tensor = torch.rand(...)\n",
    "        output = ...\n",
    "        return output\n",
    "\n",
    "# it is also highly recommended to write some simple test to ensure,\n",
    "# that this layer indeed nullifies input with probability `drop_prob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_size, dropout=0.1, attention_dropout=0.1,\n",
    "                 drop_path_rate=0.1):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.attention_pre_norm = ...\n",
    "        self.attention = torch.nn.MultiheadAttention(...)       # Do not forget to set `batch_first` parameter\n",
    "        self.attention_output_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mlp_pre_norm = ...\n",
    "        self.mlp = create_mlp(...)\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first block\n",
    "        y = self.attention_pre_norm(x)\n",
    "        attention = self.attention(y, y, y)[0]\n",
    "        attention = self.attention_output_dropout(attention)\n",
    "        x = x + self.drop_path(attention)   # Residual connection\n",
    "            \n",
    "        # second block\n",
    "        y = self.mlp_pre_norm(x)\n",
    "        y = self.mlp(y)\n",
    "        x = x + self.drop_path(y)  # Residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final class for the CCT. It looks the same as `VisionTransformer` class in seminar notebook, except the custom tokenizer and pooling. Here we implement a simple version of CCT, whose Tokenizer consist of one convolution of 3x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompactConvTransformer3x1(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_height, input_width,\n",
    "                 n_tokens,\n",
    "                 n_input_channels,\n",
    "                 embedding_dim,\n",
    "                 num_layers,\n",
    "                 num_heads=4,\n",
    "                 num_classes=10,\n",
    "                 mlp_ratio=2,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Tokenizer\n",
    "        pooling_stride = 2\n",
    "        self.tokenizer = TokenizerCCT(kernel_size=3, stride=1, padding=1, \n",
    "                                      pooling_kernel_size=3, pooling_stride=pooling_stride, pooling_padding=1,\n",
    "                                      n_output_channels=embedding_dim)\n",
    "        n_tokens = input_height // pooling_stride\n",
    "        \n",
    "        # 2. Positional embeddings\n",
    "        self.positional_embeddings = torch.nn.Parameter(\n",
    "            torch.empty((1, n_tokens*n_tokens, embedding_dim)), requires_grad=True)\n",
    "        torch.nn.init.trunc_normal_(self.positional_embeddings, std=0.2)\n",
    "        \n",
    "        # 3. TransformerEncoder with DropPath\n",
    "        mlp_size = int(embedding_dim * mlp_ratio)\n",
    "        layers_drop_path_rate = [x.item() for x in torch.linspace(0, stochastic_depth, num_layers)]\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerEncoder(\n",
    "                embedding_dim, num_heads, mlp_size, \n",
    "                dropout=dropout, attention_dropout=attention_dropout,\n",
    "                drop_path_rate=layers_drop_path_rate[i]) \n",
    "            for i in range(num_layers)])\n",
    "        \n",
    "        # 4. normalization before pooling\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # 5. sequence pooling\n",
    "        self.pool = SeqPooling(embedding_dim)\n",
    "\n",
    "        # 6. layer for the final prediction\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE\n",
    "        # 1. apply tokenizer to x\n",
    "        patch_embeddings = ...\n",
    "        \n",
    "        # 2. add position embeddings\n",
    "        x = patch_embeddings + self.positional_embeddings\n",
    "\n",
    "        # 3. apply transformer encoder blocks\n",
    "        for block in self.blocks:\n",
    "            x = ...\n",
    "            \n",
    "        # 4. apply self.norm\n",
    "        x = ...\n",
    "        \n",
    "        # 5. apply sequence pooling\n",
    "        x = ...\n",
    "        \n",
    "        # 6. final prediction\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final training \n",
    "\n",
    "If everything was implemented correctly, the following code will give you a model with > 85.5% accuracy (in fact it should be ~87.5%). If you see a smaller number, check your implementation of modules above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "model = CompactConvTransformer3x1(input_height=32, input_width=32, n_tokens=16, n_input_channels=3, \n",
    "                                  embedding_dim=128, num_layers=4, num_heads=4, num_classes=10, mlp_ratio=2)\n",
    "model = model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "scheduler = StepLRWithWarmup(opt, 40, gamma=0.3, warmup_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "num_epochs = 120 # total amount of full passes over training data\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    \n",
    "    for X_batch, y_batch in train_batch_gen:\n",
    "        opt.zero_grad()\n",
    "        # train on batch\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        logits = model(X_batch)\n",
    "        loss = compute_loss(logits, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss.append(loss.data.numpy())\n",
    "    print(scheduler.get_last_lr())\n",
    "    scheduler.step()\n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in val_batch_gen:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.numpy()\n",
    "        logits = model(X_batch).cpu()\n",
    "        y_pred = logits.max(1)[1].data.numpy()\n",
    "        val_accuracy.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss)))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
