{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81dad575",
   "metadata": {},
   "source": [
    "# Unsupervised Monocular Depth Estimation with Left-Right Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae3458",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://user-images.githubusercontent.com/20357655/144081326-a5eefc0e-3492-44d6-b7c6-95e4cf5df348.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87c11b",
   "metadata": {},
   "source": [
    "In this homework we will try to train a neural network for `depth estimation` from an image without explicit annotation (a disparity map or something else) as opposed to `DispNet` from the seminar.\n",
    "\n",
    "Method exploit epipolar geometry to generate disparity images by training our network with an image reconstruction loss. Training loss enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9f0d3",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1609.03677.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e1826",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eac3ec",
   "metadata": {},
   "source": [
    "Our training data consisted of stereo pairs from the `KITTI` dataset. We will take some of the data from the seminar, and the rest we will download additionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f34c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gfile\n",
    "gfile.download_list(\n",
    "    'https://drive.google.com/file/d/12zitJCsOVmoCHII5Ym_t2AAORXb6WMyU',\n",
    "    filename='kitti_stereo_2012_training_data.zip',\n",
    "    target_dir='.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q ./kitti_stereo_2012_training_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ec4e4",
   "metadata": {},
   "source": [
    "The directories `kitti_stereo_2012_training_data/train/colored_0` and `kitti_stereo_2012_training_data/train/colored_1` contain left and right images respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d9e16",
   "metadata": {},
   "source": [
    "## Download additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir kitti_stereo_2015\n",
    "!wget -O kitti_stereo_2015/part_1.zip \\\n",
    "    https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0032/2011_09_26_drive_0032_sync.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip kitti_stereo_2015/part_1.zip -d kitti_stereo_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d03dce",
   "metadata": {},
   "source": [
    "The directories `kitti_stereo_2015/2011_09_26/2011_09_26_drive_0032_sync/image_02/data/*.png` and `kitti_stereo_2015/2011_09_26/2011_09_26_drive_0032_sync/image_03/data/*.png` contain left and right images respectively.\n",
    "\n",
    "So, let's copy images in first dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp kitti_stereo_2015/2011_09_26/2011_09_26_drive_0032_sync/image_02/data/*.png \\\n",
    "    kitti_stereo_2012_training_data/train/colored_0/\n",
    "\n",
    "!cp kitti_stereo_2015/2011_09_26/2011_09_26_drive_0032_sync/image_03/data/*.png \\\n",
    "    kitti_stereo_2012_training_data/train/colored_1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1f96c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aadeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple, List\n",
    "\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "from functools import partial\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from PIL import Image\n",
    "import skimage\n",
    "import skimage.io\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfa1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIStereoRAM(Dataset):\n",
    "    def __init__(self, root, train=True, transforms=None):\n",
    "        super(KITTIStereoRAM, self).__init__()\n",
    "\n",
    "        self.root = Path(root)\n",
    "        if train:\n",
    "            self.path_to_dataset = self.root / 'train'\n",
    "        else:\n",
    "            self.path_to_dataset = self.root / 'val'\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.left_images = []\n",
    "        self.right_images = []\n",
    "        for left_image_path in tqdm((self.path_to_dataset / 'colored_0').rglob('*.png')):\n",
    "            right_image_path = left_image_path.as_posix().replace('colored_0', 'colored_1')\n",
    "            left_image_path = left_image_path.as_posix()\n",
    "\n",
    "            left_image = read_image(left_image_path)\n",
    "            right_image = read_image(right_image_path)\n",
    "\n",
    "            self.left_images.append(left_image)\n",
    "            self.right_images.append(right_image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.left_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        left_img = self.left_images[index]\n",
    "        right_img = self.right_images[index]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            left_img, right_img = self.transforms(left_img, right_img)\n",
    "        return left_img, right_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293add1",
   "metadata": {},
   "source": [
    "## Example of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058eae5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "img_name = \"000002_10.png\"\n",
    "\n",
    "plt.figure(figsize=(20, 7), dpi=100)\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Left image')\n",
    "plt.imshow(Image.open(\n",
    "    f'./kitti_stereo_2012_training_data/train/colored_0/{img_name}'))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Right image')\n",
    "plt.imshow(Image.open(\n",
    "    f'./kitti_stereo_2012_training_data/train/colored_1/{img_name}'))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Disparity')\n",
    "disp = np.array(Image.open(\n",
    "    f'./kitti_stereo_2012_training_data/train/disp_noc/{img_name}'))\n",
    "plt.imshow(disp / 255, \"plasma\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('Valid disparity mask')\n",
    "plt.imshow(disp > 0, \"plasma\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3aa1d",
   "metadata": {
    "code_folding": [
     49
    ]
   },
   "outputs": [],
   "source": [
    "def pad_to_size(images, min_height, min_width):\n",
    "    if images.shape[1] < min_height:\n",
    "        images = torchvision.transforms.functional.pad(\n",
    "            images, (0, 0, 0, min_height-images.shape[1]),\n",
    "        )\n",
    "    if images.shape[2] < min_width:\n",
    "        images = torchvision.transforms.functional.pad(\n",
    "            images, (0, 0, min_width - images.shape[2], 0),\n",
    "        )\n",
    "    return images\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    # torch.uint8 -> torch.float32, [0; 1]\n",
    "    return image / 255.\n",
    "\n",
    "\n",
    "color_jitter = torchvision.transforms.ColorJitter(0.2, 0.2, 0.2, 0.2)\n",
    "\n",
    "# min kitti shape is [370, 1226], max shape is [376, 1242]\n",
    "PAD_HEIGHT = 128 * 3\n",
    "PAD_WIDTH = 1280\n",
    "\n",
    "RESIZES_HEIGHT = PAD_HEIGHT\n",
    "RESIZES_WIDTH = PAD_WIDTH\n",
    "\n",
    "\n",
    "def transforms_train(left_image, right_image):\n",
    "    left_image = color_jitter(left_image)\n",
    "    right_image = color_jitter(right_image)\n",
    "    if random.random() < 0.5:\n",
    "        left_image = torchvision.transforms.functional.hflip(left_image)\n",
    "        right_image = torchvision.transforms.functional.hflip(right_image)\n",
    "\n",
    "    left_image = pad_to_size(left_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "    right_image = pad_to_size(right_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "\n",
    "    left_image = torchvision.transforms.functional.resize(\n",
    "        left_image, (RESIZES_HEIGHT, RESIZES_WIDTH)\n",
    "    )\n",
    "    right_image = torchvision.transforms.functional.resize(\n",
    "        right_image, (RESIZES_HEIGHT, RESIZES_WIDTH)\n",
    "    )\n",
    "    return normalize(left_image), normalize(right_image)\n",
    "\n",
    "\n",
    "def transforms_test(left_image, right_image):\n",
    "    left_image = pad_to_size(left_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "    right_image = pad_to_size(right_image, PAD_HEIGHT, PAD_WIDTH)\n",
    "\n",
    "    left_image = torchvision.transforms.functional.resize(\n",
    "        left_image, (RESIZES_HEIGHT, RESIZES_WIDTH)\n",
    "    )\n",
    "    right_image = torchvision.transforms.functional.resize(\n",
    "        right_image, (RESIZES_HEIGHT, RESIZES_WIDTH)\n",
    "    )\n",
    "    return normalize(left_image), normalize(right_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KITTIStereoRAM(\n",
    "    root=\"./kitti_stereo_2012_training_data/\",\n",
    "    train=True, transforms=transforms_train\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, num_workers=12\n",
    ")\n",
    "\n",
    "validation_dataset = KITTIStereoRAM(\n",
    "    root=\"./kitti_stereo_2012_training_data/\",\n",
    "    train=False, transforms=transforms_test\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, batch_size=1, shuffle=False, num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c034e",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Encoder-decoder architecture with VGGnet backbone, but without BatchNormalization and MaxPooling.\n",
    "Actually architecture is inspired by DispNet from seminar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed709c6e",
   "metadata": {},
   "source": [
    "![Model](https://user-images.githubusercontent.com/20357655/144081892-91bf13fb-8143-44d2-844b-8f03b207f9fc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9dd01",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Let's implement base `ConvBlock` and `EncoderBlock`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49fde3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Sequential):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        activation: nn.Module = nn.ReLU,\n",
    "        normalization: bool = True,\n",
    "        padding: Optional[int] = None,\n",
    "    ):\n",
    "        if padding is None:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "\n",
    "        layers = [\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding,\n",
    "                bias=not normalization\n",
    "            ),\n",
    "        ]\n",
    "        if normalization:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(activation())\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self._in_channels = in_channels\n",
    "        self._out_channels = out_channels\n",
    "        self._kernel_size = kernel_size\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            ConvBlock(\n",
    "                self._in_channels, self._out_channels, self._kernel_size,\n",
    "                stride=2, activation=partial(nn.ELU, inplace=True)\n",
    "            ),\n",
    "            ConvBlock(\n",
    "                self._out_channels, self._out_channels, self._kernel_size,\n",
    "                stride=1, activation=partial(nn.ELU, inplace=True)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return self.convs(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902cf7aa",
   "metadata": {},
   "source": [
    "## Decode Block\n",
    "\n",
    "It looks like below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a790c01",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/20357655/144682874-38496b10-e5a3-4455-af9d-2cdc375a1ae5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94acc607",
   "metadata": {},
   "source": [
    "More precisely, first three block consist of:\n",
    "   1. Up-Convolution (Upsample + Conv)\n",
    "   2. Concat current feature map (from 1) with skip-connection from encoder\n",
    "   3. Convolution\n",
    "    \n",
    "Last four block are complement by `DisparityHead` and we predict disparity maps. So, in addition to the previous computations, we additionally predict two disparity maps (for left and right image). And these maps will later be used to calculate the loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91fe78d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DisparityHead(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, disparity_max: float):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels=2,\n",
    "                              kernel_size=3, padding=1)\n",
    "        self._disparity_max = disparity_max\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        # you should scale output by sigmoid and mutliply by self._disparity_max\n",
    "        # TODO\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        scale: float = 2.0,\n",
    "        with_skip: bool = False,\n",
    "        use_previous_disparity: bool = False,\n",
    "        with_disparity_head: bool = False,\n",
    "        disparity_max: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._in_channels = in_channels\n",
    "        self._out_channels = out_channels\n",
    "        self._kernel_size = kernel_size\n",
    "        self._scale = scale\n",
    "\n",
    "        self.with_skip = with_skip\n",
    "        self.use_previous_disparity = use_previous_disparity\n",
    "        self.with_disparity_head = with_disparity_head\n",
    "\n",
    "        self.up_conv = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=self._scale, mode='nearest'),\n",
    "            ConvBlock(\n",
    "                self._in_channels, self._out_channels, self._kernel_size,\n",
    "                activation=partial(nn.ELU, inplace=True), normalization=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        in_channels = self._out_channels\n",
    "        if self.with_skip:\n",
    "            in_channels *= 2\n",
    "        if self.use_previous_disparity:\n",
    "            in_channels += 2\n",
    "        self.conv = ConvBlock(\n",
    "            in_channels, self._out_channels,\n",
    "            self._kernel_size, activation=partial(nn.ELU, inplace=True)\n",
    "        )\n",
    "\n",
    "        if self.with_disparity_head:\n",
    "            assert disparity_max is not None\n",
    "            self.disparity_head = DisparityHead(\n",
    "                self._out_channels, disparity_max\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        skip_input: Optional[torch.Tensor] = None,\n",
    "        previous_disparity_map: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_head = DisparityHead(128, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b96c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert disp_head(torch.randn(1, 128, 10, 10)).max() <= 0.3 \\\n",
    "    and disp_head(torch.randn(1, 128, 10, 10)).max() >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_block = DecoderBlock(64, 128)\n",
    "assert decoder_block(torch.randn(1, 64, 10, 10)).shape[1:] == (128, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_block = DecoderBlock(64, 128, with_skip=True)\n",
    "assert decoder_block(\n",
    "    torch.randn(1, 64, 10, 10), torch.randn(1, 128, 20, 20)\n",
    ").shape[1:] == (128, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52983235",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_block = DecoderBlock(64, 128, with_skip=True, use_previous_disparity=True)\n",
    "assert decoder_block(\n",
    "    torch.randn(1, 64, 10, 10), torch.randn(1, 128, 20, 20), torch.randn(1, 2, 20, 20)\n",
    ").shape[1:] == (128, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a05ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_block = DecoderBlock(64, 128, with_skip=True, with_disparity_head=True, disparity_max=0.3)\n",
    "feature_map, disparity_map = decoder_block(\n",
    "    torch.randn(1, 64, 10, 10), torch.randn(1, 128, 20, 20)\n",
    ")\n",
    "assert disparity_map.shape[1:] == (2, 20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef4d3b",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/20357655/144083678-bf059091-6884-44eb-bcb8-1e61de47c910.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e37e8",
   "metadata": {},
   "source": [
    "Here we detail the configuration of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455889a",
   "metadata": {
    "code_folding": [
     1,
     8,
     20,
     21
    ]
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EncoderBlockConfig:\n",
    "    in_channels: int\n",
    "    out_channels: int\n",
    "    kernel_size: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecoderBlockConfig:\n",
    "    in_channels: int\n",
    "    out_channels: int\n",
    "    kernel_size: int = 3\n",
    "    scale: float = 2.0\n",
    "    with_skip: bool = False\n",
    "    use_previous_disparity: bool = False\n",
    "    with_disparity_head: bool = False\n",
    "    disparity_max: Optional[float] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DisparityNetConfig:\n",
    "    encoder: Tuple[EncoderBlockConfig, ...] = (\n",
    "        EncoderBlockConfig(3, 32, 7),\n",
    "        EncoderBlockConfig(32, 64, 5),\n",
    "        EncoderBlockConfig(64, 128, 3),\n",
    "        EncoderBlockConfig(128, 256, 3),\n",
    "        EncoderBlockConfig(256, 512, 3),\n",
    "        EncoderBlockConfig(512, 512, 3),\n",
    "        EncoderBlockConfig(512, 512, 3)\n",
    "    )\n",
    "    \n",
    "    # from bottom to top\n",
    "    disparity_maxes: Tuple[float, ...] = (\n",
    "        0.3, 0.3, 0.3, 0.3\n",
    "    )\n",
    "    image_scale = 1\n",
    "    decoder: Tuple[DecoderBlock, ...] = (\n",
    "        DecoderBlockConfig(512, 512, with_skip=True),\n",
    "        DecoderBlockConfig(512, 512, with_skip=True),\n",
    "        DecoderBlockConfig(512, 256, with_skip=True),\n",
    "        DecoderBlockConfig(\n",
    "            256, int(128 * image_scale), with_skip=True,\n",
    "            with_disparity_head=True,\n",
    "            disparity_max=disparity_maxes[0]\n",
    "        ),\n",
    "        DecoderBlockConfig(\n",
    "            128, int(64 * image_scale), with_skip=True,\n",
    "            with_disparity_head=True,\n",
    "            use_previous_disparity=True,\n",
    "            disparity_max=disparity_maxes[1]\n",
    "        ),\n",
    "        DecoderBlockConfig(\n",
    "            64, int(32 * image_scale), with_skip=True,\n",
    "            with_disparity_head=True,\n",
    "            use_previous_disparity=True,\n",
    "            disparity_max=disparity_maxes[2]\n",
    "        ),\n",
    "        DecoderBlockConfig(\n",
    "            32, int(16 * image_scale),\n",
    "            with_disparity_head=True,\n",
    "            use_previous_disparity=True,\n",
    "            disparity_max=disparity_maxes[3]\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbf731",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DisparityNet(nn.Module):\n",
    "    # we have strong assumption about network topology!\n",
    "\n",
    "    def __init__(self, config: DisparityNetConfig):\n",
    "        super().__init__()\n",
    "        self._config = config\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            EncoderBlock(**asdict(encoder_config))\n",
    "            for encoder_config in self._config.encoder\n",
    "        ])\n",
    "\n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecoderBlock(**asdict(decoder_config))\n",
    "            for decoder_config in self._config.decoder\n",
    "        ])\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity_net_config = DisparityNetConfig()\n",
    "model = DisparityNet(disparity_net_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity_maps = model(torch.randn(1, 3, PAD_HEIGHT, PAD_WIDTH))\n",
    "assert len(disparity_maps) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a8c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, disparity_map in enumerate(disparity_maps):\n",
    "    target_shape = (2, PAD_HEIGHT // (2 ** (3 - i)), PAD_WIDTH // (2 ** (3 - i)))\n",
    "    assert disparity_map.shape[1:] == target_shape, f\"get {disparity_map.shape[1:]} but expect {target_shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15cdba",
   "metadata": {},
   "source": [
    "## Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414e033b",
   "metadata": {},
   "source": [
    "### `Left-Right Disparity Consistency`\n",
    "\n",
    "Loss To produce more accurate disparity maps, we train our network to predict both the left and right image disparities, while only being given the left view as input to the convolutional part of the network. To ensure coherence, we introduce an $L_1$ left-right disparity consistency penalty as part of our model. This cost attempts to make the left-view disparity map be equal to the projected right-view disparity map,\n",
    "\n",
    "$$\n",
    "C_{l r}^{l}=\\frac{1}{N} \\sum_{i, j}\\left|d_{i j}^{l}-d_{i j+d_{i j}^{l}}^{r}\\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b90bd7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def apply_disparity(image, disparity_map):\n",
    "    batch_size, _, height, width = image.size()\n",
    "\n",
    "    # Original coordinates of pixels\n",
    "    x_base = torch.linspace(0, 1, width).repeat(\n",
    "        batch_size, height, 1).type_as(image)\n",
    "    y_base = torch.linspace(0, 1, height).repeat(\n",
    "        batch_size, width, 1).transpose(1, 2).type_as(image)\n",
    "\n",
    "    # Apply shift in X direction\n",
    "    x_shifts = disparity_map[:, 0, :, :]\n",
    "    flow_field = # TODO\n",
    "\n",
    "    # Apply F.grid_sample for `image` and `flow_field`\n",
    "    # Note that in grid_sample coordinates are assumed to be between -1 and 1\n",
    "    # TODO\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class DisparityConsistencyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, left_disparity_map, right_disparity_map):\n",
    "        # TODO\n",
    "        # apply left_disparity_map to right_disparity_map and calculate consistency loss (L1)\n",
    "        # and vice verse\n",
    "\n",
    "        # Pay attention that direction of maps are different !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e1835",
   "metadata": {},
   "source": [
    "### `Disparity Smoothness Loss`\n",
    "\n",
    "We encourage disparities to be locally smooth with an $L_1$ penalty on the disparity gradients $\\partial d$. As depth discontinuities often occur at image gradients, we weight this cost with an edge-aware term using the image gradients $\\partial I$,\n",
    "\n",
    "$$\n",
    "C_{d s}^{l}=\\frac{1}{N} \\sum_{i, j}\\left|\\partial_{x} d_{i j}^{l}\\right| e^{-\\left\\|\\partial_{x} I_{i j}^{l}\\right\\|}+\\left|\\partial_{y} d_{i j}^{l}\\right| e^{-\\left\\|\\partial_{y} I_{i j}^{l}\\right\\|} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4132504",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DisparitySmoothnessLoss(nn.Module):\n",
    "    \n",
    "    @staticmethod\n",
    "    def x_grad(image):\n",
    "        image = F.pad(image, (0, 1, 0, 0), mode='replicate')\n",
    "        grad_x = image[:, :, :, :-1] - image[:, :, :, 1:]\n",
    "        return grad_x\n",
    "\n",
    "    @staticmethod\n",
    "    def y_grad(image):\n",
    "        image = F.pad(image, (0, 0, 0, 1), mode='replicate')\n",
    "        grad_y = image[:, :, :-1, :] - image[:, :, 1:, :]\n",
    "        return grad_y\n",
    "\n",
    "    \n",
    "    def forward(self, disparity_map, image):\n",
    "        # TODO\n",
    "        # just implement above formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5315c5eb",
   "metadata": {},
   "source": [
    "### `Appearance Matching Loss`\n",
    "\n",
    "We use a combination of an $L_1$ and single scale SSIM term as our photometric image reconstruction cost $C_{a p}$, which compares the input image $I_{i j}^{l}$ and its reconstruction $\\tilde{I}_{i j}^{l}$, where $N$ is the number of pixels,\n",
    "\n",
    "$$\n",
    "C_{a p}^{l}=\\frac{1}{N} \\sum_{i, j} \\alpha \\frac{1-\\operatorname{SSIM}\\left(I_{i j}^{l}, \\tilde{I}_{i j}^{l}\\right)}{2}+(1-\\alpha)\\left\\|I_{i j}^{l}-\\tilde{I}_{i j}^{l}\\right\\| .\n",
    "$$\n",
    "\n",
    "Here, we use a simplified SSIM with a $3 \\times 3$ block filter instead of a Gaussian, and set $\\alpha=0.85$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eba70c",
   "metadata": {},
   "source": [
    "The SSIM index is calculated on various windows of an image. The measure between two windows $x$ and $y$ of common size $N \\times N$ is:\n",
    "$$\n",
    "\\operatorname{SSIM}(x, y)=\\frac{\\left(2 \\mu_{x} \\mu_{y}+c_{1}\\right)\\left(2 \\sigma_{x y}+c_{2}\\right)}{\\left(\\mu_{x}^{2}+\\mu_{y}^{2}+c_{1}\\right)\\left(\\sigma_{x}^{2}+\\sigma_{y}^{2}+c_{2}\\right)}\n",
    "$$\n",
    "with:\n",
    "- $\\mu_{x}$ the average of $x$;\n",
    "- $\\mu_{y}$ the average of $y$;\n",
    "- $\\sigma_{x}^{2}$ the variance of $x$;\n",
    "- $\\sigma_{y}^{2}$ the variance of $y$;\n",
    "- $\\sigma_{x y}$ the covariance of $x$ and $y$;\n",
    "- $c_{1}=\\left(k_{1} L\\right)^{2}, c_{2}=\\left(k_{2} L\\right)^{2}$ two variables to stabilize the division with weak denominator;\n",
    "- $L$ the dynamic range of the pixel-values (typically this is $2^{\\# b i t s \\text { per pixel }-1 \\text { ); }}$\n",
    "- $k_{1}=0.01$ and $k_{2}=0.03$ by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca7363",
   "metadata": {
    "code_folding": [
     12
    ]
   },
   "outputs": [],
   "source": [
    "def gaussian(window_size: int, sigma: float):\n",
    "    def gauss_fcn(x):\n",
    "        return -(x - window_size // 2) ** 2 / float(2 * sigma ** 2)\n",
    "    gauss = torch.stack([\n",
    "        torch.exp(torch.tensor(gauss_fcn(x))) for x in range(window_size)\n",
    "    ])\n",
    "    return gauss / gauss.sum()\n",
    "\n",
    "\n",
    "def get_gaussian_kernel2d(\n",
    "    ksize: Tuple[int, int],\n",
    "    sigma: Tuple[float, float]\n",
    ") -> torch.Tensor:\n",
    "    ksize_x, ksize_y = ksize\n",
    "    sigma_x, sigma_y = sigma\n",
    "    kernel_x: torch.Tensor = gaussian(ksize_x, sigma_x)\n",
    "    kernel_y: torch.Tensor = gaussian(ksize_y, sigma_y)\n",
    "    kernel_2d: torch.Tensor = torch.matmul(\n",
    "        kernel_x.unsqueeze(-1), kernel_y.unsqueeze(-1).t())\n",
    "    return kernel_2d\n",
    "\n",
    "\n",
    "class DSSIMLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, window_size: int, max_val: float = 1.0):\n",
    "        super().__init__()\n",
    "        self._window_size = window_size\n",
    "        self._max_val = max_val\n",
    "\n",
    "        self.window = get_gaussian_kernel2d(\n",
    "            (window_size, window_size), (1.5, 1.5))\n",
    "        self.padding = (window_size - 1) // 2\n",
    "\n",
    "        self.C1: float = (0.01 * self._max_val) ** 2\n",
    "        self.C2: float = (0.03 * self._max_val) ** 2\n",
    "\n",
    "    def filter2D(\n",
    "        self, input: torch.Tensor, kernel: torch.Tensor, channel: int\n",
    "    ) -> torch.Tensor:\n",
    "        return F.conv2d(input, kernel, padding=self.padding, groups=channel)\n",
    "\n",
    "    def forward(self, image1, image2):\n",
    "        b, c, h, w = image1.shape\n",
    "        tmp_kernel = self.window.to(image1.device).to(image1.dtype)\n",
    "        kernel = tmp_kernel.repeat(c, 1, 1, 1)\n",
    "\n",
    "        # compute local mean per channel\n",
    "        mu1 = self.filter2D(image1, kernel, c)\n",
    "        mu2 = self.filter2D(image2, kernel, c)\n",
    "\n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        # compute local sigma per channel\n",
    "        sigma1_sq = self.filter2D(image1 * image1, kernel, c) - mu1_sq\n",
    "        sigma2_sq = self.filter2D(image2 * image2, kernel, c) - mu2_sq\n",
    "        sigma12 = self.filter2D(image1 * image2, kernel, c) - mu1_mu2\n",
    "\n",
    "        ssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\\n",
    "            ((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))\n",
    "\n",
    "        loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class AppearanceMatchingLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, alpha: float):\n",
    "        super().__init__()\n",
    "        self._alpha = alpha\n",
    "\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.dssim_loss = DSSIMLoss(3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        left_disparity_map,\n",
    "        right_disparity_map,\n",
    "        left_image,\n",
    "        right_image\n",
    "    ):\n",
    "        # TODO\n",
    "        # reconstruct left image with help of right image and left disparity map\n",
    "        # and vice verse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07feb34",
   "metadata": {},
   "source": [
    "We define a loss $C_{s}$ at each output scale s, forming the total loss as the sum $C=\\sum_{s=1}^{4} C_{s}$. Our loss module (or decoder block) computes $C_{s}$ as a combination of three main terms,\n",
    "\n",
    "$$\n",
    "C_{s}=\\alpha_{a p}\\left(C_{a p}^{l}+C_{a p}^{r}\\right)+\\alpha_{d s}\\left(C_{d s}^{l}+C_{d s}^{r}\\right)+\\alpha_{l r}\\left(C_{l r}^{l}+C_{l r}^{r}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f997169b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MultiScaleDisparityLoss(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        appearance_alpha: float = 1.0,\n",
    "        consistency_alpha: float = 1.0,\n",
    "        scales: Tuple[int, ...] = (1, )\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._appearance_alpha = appearance_alpha\n",
    "        self._consistency_alpha = consistency_alpha\n",
    "        self._scales = scales\n",
    "        self._smoothness_alphas = [\n",
    "            0.1 / scale\n",
    "            for scale in self._scales\n",
    "        ]\n",
    "\n",
    "        self.consistency_loss = DisparityConsistencyLoss()\n",
    "        self.smoothness_loss = DisparitySmoothnessLoss()\n",
    "        self.appearance_loss = AppearanceMatchingLoss(0.85)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        left_image: torch.Tensor,\n",
    "        right_image: torch.Tensor,\n",
    "        disparity_maps: Tuple[torch.Tensor, ...]\n",
    "    ):\n",
    "        assert len(disparity_maps) == len(self._scales)\n",
    "\n",
    "        loss = 0\n",
    "        for i, (current_scale, disparity_map) in enumerate(zip(self._scales, disparity_maps)):\n",
    "            left_disparity_map, right_disparity_map = disparity_map.split(1, 1)\n",
    "            # TODO\n",
    "            # resize left and right images according to current scale\n",
    "            # and calculate consistency, smoothness and appearance criterions \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2aad7",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In order for the work to be accepted, you must achieve a quality of ~0.725 (loss value) and visualize several samples as in the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "criterion = MultiScaleDisparityLoss(\n",
    "    scales=(8, 4, 2, 1),\n",
    "    consistency_alpha=1.0,\n",
    ").to(device)\n",
    "\n",
    "model = DisparityNet(DisparityNetConfig()).to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14871986",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_network(network, opt, criterion, num_epochs=200):\n",
    "    metrics = defaultdict(list)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        train_scale_losses = []\n",
    "        val_scale_losses = []\n",
    "\n",
    "        network.train()\n",
    "        for left_image, right_image in tqdm.tqdm(train_dataloader):\n",
    "            opt.zero_grad()\n",
    "\n",
    "            left_image = left_image.to(device)\n",
    "            right_image = right_image.to(device)\n",
    "\n",
    "            disparity_maps = network(left_image)\n",
    "\n",
    "            loss = criterion(\n",
    "                left_image, right_image, disparity_maps\n",
    "            )\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        network.eval()\n",
    "        with torch.no_grad():\n",
    "            for left_image, right_image in tqdm.tqdm(validation_dataloader):\n",
    "                left_image = left_image.to(device)\n",
    "                right_image = right_image.to(device)\n",
    "\n",
    "                disparity_maps = network(left_image)\n",
    "\n",
    "                loss = criterion(\n",
    "                    left_image, right_image, disparity_maps\n",
    "                )\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "        metrics['train_loss'].append(np.mean(train_loss))\n",
    "        metrics['val_loss'].append(np.mean(val_loss))\n",
    "\n",
    "        display.clear_output()\n",
    "        plt.figure(figsize=(20, 7))\n",
    "        ax1 = plt.subplot(2, 2, 1)\n",
    "        ax1.imshow(disparity_maps[-1][0, 0].cpu(), \"plasma\")\n",
    "        \n",
    "        ax2 = plt.subplot(2, 2, 3)\n",
    "        ax2.imshow(left_image.cpu()[0].permute(1, 2, 0))\n",
    "        \n",
    "        ax3 = plt.subplot(1, 2, 2)\n",
    "        ax3.plot(metrics['train_loss'], label='train')\n",
    "        ax3.plot(metrics['val_loss'], label='val')\n",
    "        ax3.set_xlabel('Epochs', fontsize=20)\n",
    "        ax3.set_ylabel('Loss', fontsize=20)\n",
    "        ax3.grid()\n",
    "        ax3.legend()\n",
    "        \n",
    "        plt.show()\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} of {num_epochs} \"\n",
    "            f\"took {time.time() - start_time:.3f}s\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc3491",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_network(\n",
    "    model, opt, criterion, 200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aead24",
   "metadata": {},
   "source": [
    "## Visualize some results from validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d42cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 5, 10, 11]:\n",
    "    left_image, right_image = validation_dataset[i]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        disp_maps = model(left_image[None].cuda())\n",
    "        last_map = disp_maps[-1][0, 0].cpu().numpy()[:-30, :-100]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 7), dpi=100)\n",
    "    axes[0].imshow(left_image.permute(1, 2, 0)[:-30, :-100])\n",
    "    axes[1].imshow(last_map)\n",
    "    axes[0].axis('off'); axes[1].axis('off');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75875cbd",
   "metadata": {},
   "source": [
    "# Quality benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a708d1",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/20357655/144706889-2c6f31f1-7715-4ea1-b53a-025f8fe7e70c.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16c350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
