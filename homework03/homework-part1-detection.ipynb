{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxpQrOmpnfFA"
   },
   "source": [
    "# Homework 3 - pt 1 - Object Detectors\n",
    "\n",
    "This homework is a continuation of the seminar. You need first to complete all the gaps that were completed on the seminar, and then do the homework part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "884a6408"
   },
   "source": [
    "# Seminar part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b68b9da2"
   },
   "source": [
    "Today is a good day to learn how to implement your own object detector!\n",
    "\n",
    "Being honest, modern object detectors are still be very complicated systems with a lot of important tricks which help neural network to perform better. So the best strategy for obj detector implementation is to take a good open-source implementation as a basis and incrementally refactor it.\n",
    "\n",
    "However all implementations have some common things in it. The goal of the current seminar is to learn that things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b69d21a4"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "502eefea"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "def get_computing_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_computing_device()\n",
    "print(f\"Our main computing device is '{device}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6855a328"
   },
   "source": [
    "## 1. Pascal VOC 2012\n",
    "\n",
    "20 classes, 5717 images in train, 5823 images in validation.\n",
    "\n",
    "[[homepage]](http://host.robots.ox.ac.uk/pascal/VOC/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a99d9c92"
   },
   "outputs": [],
   "source": [
    "CLASSES = (\n",
    "        \"__background__ \",\n",
    "        \"aeroplane\",\n",
    "        \"bicycle\",\n",
    "        \"bird\",\n",
    "        \"boat\",\n",
    "        \"bottle\",\n",
    "        \"bus\",\n",
    "        \"car\",\n",
    "        \"cat\",\n",
    "        \"chair\",\n",
    "        \"cow\",\n",
    "        \"diningtable\",\n",
    "        \"dog\",\n",
    "        \"horse\",\n",
    "        \"motorbike\",\n",
    "        \"person\",\n",
    "        \"pottedplant\",\n",
    "        \"sheep\",\n",
    "        \"sofa\",\n",
    "        \"train\",\n",
    "        \"tvmonitor\",\n",
    "    )\n",
    "LABEL_TO_CLASS_INDEX = {k:i for i,k in enumerate(CLASSES)}\n",
    "CLASS_INDEX_TO_LABEL = {i:k for i,k in enumerate(CLASSES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43d5d07b"
   },
   "outputs": [],
   "source": [
    "means = np.array([0.45704785, 0.43824798, 0.40617362], dtype=np.float32)\n",
    "stds = np.array([0.23908612, 0.23509602, 0.2397311 ], dtype=np.float32)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "def transform_targets(targets):\n",
    "    targets = targets['annotation']\n",
    "    objects = targets['object']\n",
    "    classes = [LABEL_TO_CLASS_INDEX[elem['name']] for elem in objects]\n",
    "    objects = [[int(elem['bndbox']['xmin']), int(elem['bndbox']['ymin']), \n",
    "                int(elem['bndbox']['xmax']), int(elem['bndbox']['ymax'])] for elem in objects]\n",
    "    sizes = [(elem[2] - elem[0]) * (elem[3] - elem[1]) for elem in objects]\n",
    "    order = np.argsort(sizes)[::-1]\n",
    "    objects = np.array(objects, np.int32)[order]\n",
    "    classes = np.array(classes, np.int32)[order]\n",
    "    return {'xyxy': torch.from_numpy(objects), \n",
    "            'classes': torch.from_numpy(classes)}\n",
    "\n",
    "MIN_HEIGHT = 512\n",
    "MIN_WIDTH = 512\n",
    "def transforms_train(images, targets):\n",
    "    targets = transform_targets(targets)\n",
    "    images = transform_train(images)\n",
    "    images = pad_to_size(images, MIN_HEIGHT, MIN_WIDTH)\n",
    "    return images, targets\n",
    "\n",
    "def pad_to_size(images, min_height, min_width):\n",
    "    if images.shape[1] < min_height:\n",
    "        images = torchvision.transforms.functional.pad(images, (0,0,0,min_height-images.shape[1]))\n",
    "    if images.shape[2] < min_width:\n",
    "        images = torchvision.transforms.functional.pad(images, (0,0, min_width - images.shape[2], 0))\n",
    "    return images\n",
    "        \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "def transforms_test(images, targets):\n",
    "    targets = transform_targets(targets)\n",
    "    images = transform_test(images)\n",
    "    images = pad_to_size(images, MIN_HEIGHT, MIN_WIDTH)\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "940bad62"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import VOCDetection\n",
    "train_loader = VOCDetection(root=\"./voc_data/\", download=True, image_set='train', transforms=transforms_train)\n",
    "\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_loader, \n",
    "                                              batch_size=1,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=16)\n",
    "val_loader = VOCDetection(root=\"./voc_data/\", download=True, image_set='val', transforms=transforms_test)\n",
    "\n",
    "val_batch_gen = torch.utils.data.DataLoader(val_loader, \n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e70e759",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_images = 6\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.tight_layout()\n",
    "for i, (x,y) in enumerate(val_batch_gen):\n",
    "    if i == n_images:\n",
    "        break\n",
    "    plt.subplot(3, n_images // 3, i+1)\n",
    "    img = x[0]*stds.reshape(3,1,1) + means.reshape(3,1,1)\n",
    "    img = img*255\n",
    "    img = img.type(torch.uint8)\n",
    "    labels = [CLASS_INDEX_TO_LABEL[ind.numpy().tolist()] for ind in y['classes'][0]]\n",
    "    img = torchvision.utils.draw_bounding_boxes(img, y['xyxy'][0], width=2, labels=labels)\n",
    "    plt.imshow(img.numpy().transpose(1,2,0))\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f55643d1"
   },
   "source": [
    "## 2. FCOS - Fully Convolutional One-Stage Object Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7395943"
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1904.01355v5.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b61c29e"
   },
   "source": [
    "### 2.1 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ecae528"
   },
   "source": [
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-23_at_3.34.09_PM_SAg1OBo.png\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4e0f1d4"
   },
   "source": [
    "Let's implement our detector on the top of resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1673e827"
   },
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18685bed",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c62d987"
   },
   "outputs": [],
   "source": [
    "N_SCALES = 5\n",
    "N_HEAD_LAYERS = 4\n",
    "\n",
    "    \n",
    "\n",
    "class RegressionHead(torch.nn.Module):\n",
    "    def __init__(self, n_head_layers, n_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE: sequential with n_head_layers Conv2d(3x3 with padding) + relu\n",
    "        self.layers = torch.nn.Sequential(...)\n",
    "        \n",
    "        self.final_conv = torch.nn.Conv2d(n_features, 4, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class ClassificationHead(torch.nn.Module):\n",
    "    def __init__(self, n_head_layers, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        # YOUR CODE: same story, sequential but with two final heads\n",
    "        self.layers = torch.nn.Sequential(...)\n",
    "            \n",
    "        self.cls_pred = torch.nn.Conv2d(n_features, n_classes, kernel_size=1)\n",
    "        self.centerness_pred = torch.nn.Conv2d(n_features, 1, kernel_size=1)\n",
    "        \n",
    "        # YOUR CODE: init self.cls_pred.bias with constant b so that sigmoid(b) = 0.01\n",
    "        init_prob = 0.01  # for focal loss\n",
    "        \n",
    "        torch.nn.init.constant_(self.cls_pred.bias, ...)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        cls = self.cls_pred(x)\n",
    "        centerness = self.centerness_pred(x)\n",
    "        return cls, centerness\n",
    "\n",
    "    \n",
    "class FCOS(torch.nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "        self.resnet18.train(False)\n",
    "        n_features = 256\n",
    "        \n",
    "        self.conv_p5 = torch.nn.Conv2d(512, n_features, kernel_size=(1,1))\n",
    "        self.conv_p6 = torch.nn.Conv2d(n_features, n_features, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv_p7 = torch.nn.Conv2d(n_features, n_features, kernel_size=(3,3), stride=2, padding=1)\n",
    "        self.conv_p4 = torch.nn.Conv2d(256, n_features, kernel_size=(1,1))\n",
    "        self.conv_p3 = torch.nn.Conv2d(128, n_features, kernel_size=(1,1))\n",
    "        \n",
    "        self.refinement_conv_p3 = torch.nn.Conv2d(n_features, n_features, kernel_size=(1,1))\n",
    "        self.refinement_conv_p4 = torch.nn.Conv2d(n_features, n_features, kernel_size=(1,1))\n",
    "        self.refinement_conv_p5 = torch.nn.Conv2d(n_features, n_features, kernel_size=(1,1))\n",
    "        self.refinement_conv_p6 = torch.nn.Conv2d(n_features, n_features, kernel_size=(1,1))\n",
    "        self.refinement_conv_p7 = torch.nn.Conv2d(n_features, n_features, kernel_size=(1,1))\n",
    "        self.refinement_convs = [self.refinement_conv_p3, self.refinement_conv_p4, self.refinement_conv_p5, \n",
    "                                 self.refinement_conv_p6, self.refinement_conv_p7]\n",
    "                                \n",
    "        self.cls_head = ClassificationHead(N_HEAD_LAYERS, n_features, n_classes)\n",
    "        self.reg_head = RegressionHead(N_HEAD_LAYERS, n_features)\n",
    "        \n",
    "        self.regression_scales = torch.nn.Parameter(torch.ones(len(self.refinement_convs), dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.resnet18.conv1(x)\n",
    "        x = self.resnet18.bn1(x)\n",
    "        x = self.resnet18.relu(x)\n",
    "        x = self.resnet18.maxpool(x)\n",
    "\n",
    "        c2 = self.resnet18.layer1(x)\n",
    "        c3 = self.resnet18.layer2(c2)\n",
    "        c4 = self.resnet18.layer3(c3)\n",
    "        c5 = self.resnet18.layer4(c4)\n",
    "        # don't compute avgpool+flatten+fc in resnet\n",
    "        \n",
    "        # YOUR CODE\n",
    "        p5 = self.conv_p5(c5)\n",
    "        p6 = ...\n",
    "        p7 = ...\n",
    "        p4 = self.conv_p4(c4) + F.interpolate(p5, size=c4.shape[2:], mode='nearest')\n",
    "        p3 = ...\n",
    "        \n",
    "        cls_results = []\n",
    "        reg_results = []\n",
    "        centerness_results = []\n",
    "        for i, scale_features in enumerate([p3, p4, p5, p6, p7]):\n",
    "            # YOUR CODE apply refinement_convs and heads\n",
    "            refined_features = ...\n",
    "            cls, centerness = ...\n",
    "            reg = self.reg_head(refined_features) * self.regression_scales[i]\n",
    "            cls_results.append(cls)\n",
    "            reg_results.append(reg)\n",
    "            centerness_results.append(centerness)\n",
    "        return cls_results, reg_results, centerness_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0133587"
   },
   "outputs": [],
   "source": [
    "detector = FCOS(n_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cddab362"
   },
   "outputs": [],
   "source": [
    "pred = detector(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0401dcb1"
   },
   "source": [
    "### 2.2 Assignment and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8c4183a"
   },
   "source": [
    "In FCOS in each pixel we predict box class label, distances to left, right, top and bottom borders of box object and centerness heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "935f2ce6"
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/225/1*8Ou4qNoKscReoOzMQJ1ZYA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4ec7cd5"
   },
   "source": [
    "It all starts with assignment of gt boxes to the pixels of sourse images. \n",
    "\n",
    "Here is a quote from the paper about assignment procedure:\n",
    "\n",
    "> More specifically, we firstly compute the regression targets $(l^∗, t^∗, r^∗, b^∗)$ for each location on all feature levels.  \n",
    "> Next,  if a location satisfies $\\max(l^∗, t^∗, r^∗, b^∗) > m_i$ or $\\max(l^∗, t^∗, r^∗, b^∗) < m_{i−1}$, it is set as a negative sample and is thus not required to regress a bounding box anymore.  \n",
    "> Here $m_i$ is the maximum distance that feature level $i$ needs to regress. In this work $m_2$,$m_3$,$m_4$,$m_5$,$m_6$ and $m_7$ are set as 0, 64, 128, 256, 512 and $\\inf$, respectively. \n",
    "> Since objects with different sizes are assigned to different feature levels and most overlapping happens between objects with considerably different sizes. \n",
    "\n",
    "> If a location is still assigned to more than one ground truth boxes, we simply choose the ground truth box with minimal area as its target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e780ad0e"
   },
   "source": [
    "Once assignment is done, we compute regression and centerness targets for pixels assigned to some box. Regression targets definition is straight forward: for each pixel it's the distance from the pixel to boundaries.\n",
    "\n",
    "Centerness is defined by specific formula:\n",
    "$$\\text{centerness} = \\sqrt{\\frac{\\min(l^*, r^*)}{\\max(l^*,r^*)} \\times \\frac{\\min(t^*, b^*)}{\\max(t^*,b^*)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b2c6f66"
   },
   "outputs": [],
   "source": [
    "def calc_centerness(scale_regression):\n",
    "    \"\"\"\n",
    "    scale_regression - tensor of shape [bs, n_boxes, 4] in form of xyxy\n",
    "    \"\"\"\n",
    "    # YOUR CODE don't forget to fix div by zero\n",
    "    scale_centerness = torch.sqrt(...)\n",
    "    return scale_centerness\n",
    "    \n",
    "    \n",
    "def calc_regression_targets(object_xyxy, object_xyxy_at_scale, scale_stride):\n",
    "    x_min, y_min, x_max, y_max = torch.unbind(object_xyxy_at_scale)\n",
    "    center_points = torch.meshgrid(torch.arange(y_min, y_max), torch.arange(x_min, x_max))[::-1]\n",
    "    center_points = torch.stack(center_points, axis=-1)\n",
    "    center_points = center_points.type(torch.float32) * scale_stride + scale_stride / 2\n",
    "    \n",
    "    # YOUR CODE \n",
    "    left_top_shift = ...\n",
    "    right_bottom_shift = ...\n",
    "    result = torch.cat((left_top_shift, right_bottom_shift), axis=-1)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def get_heads_gt(targets, shapes, strides):\n",
    "    n = len(targets['classes'][0])\n",
    "    gt_classes = []\n",
    "    gt_centerness = []\n",
    "    gt_regression = []\n",
    "    n_scales = len(shapes)\n",
    "    REGRESSION_BOUNDS = [0, 64, 128, 256, 512, 1e5]\n",
    "    \n",
    "    for i in range(n_scales):\n",
    "        h, w = shapes[i][2], shapes[i][3]\n",
    "        scale_stride = strides[i]\n",
    "        \n",
    "        scale_classes = torch.zeros([h, w], dtype=torch.int32)\n",
    "        scale_regression = torch.zeros([h,w,4], dtype=torch.float32)\n",
    "        \n",
    "        for j in range(n):\n",
    "            object_xyxy = targets['xyxy'][0, j]\n",
    "            object_xyxy_at_scale = torch.round(object_xyxy / scale_stride).type(torch.int32)\n",
    "            x_min, y_min, x_max, y_max = torch.unbind(object_xyxy_at_scale)\n",
    "            if x_min >= x_max or y_min >= y_max:\n",
    "                continue\n",
    "            regression_targets = calc_regression_targets(object_xyxy, object_xyxy_at_scale, scale_stride)\n",
    "            max_regression_targets = regression_targets.max(axis=-1)[0]\n",
    "            mask = torch.logical_and(max_regression_targets >= REGRESSION_BOUNDS[i],\n",
    "                                     max_regression_targets <= REGRESSION_BOUNDS[i+1])\n",
    "            #mask = torch.ones_like(mask)\n",
    "            scale_classes[y_min:y_max, x_min:x_max] = torch.where(\n",
    "                mask, targets['classes'][0, j], scale_classes[y_min:y_max, x_min:x_max])\n",
    "            scale_regression[y_min:y_max, x_min:x_max] = torch.where(\n",
    "                mask[:,:,np.newaxis], regression_targets, scale_regression[y_min:y_max, x_min:x_max])\n",
    "            \n",
    "        scale_centerness = calc_centerness(scale_regression)\n",
    "        gt_classes.append(scale_classes[np.newaxis, np.newaxis])\n",
    "        gt_centerness.append(scale_centerness[np.newaxis, np.newaxis])\n",
    "        gt_regression.append(scale_regression.permute(2,0,1)[np.newaxis])\n",
    "    return gt_classes, gt_regression, gt_centerness\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "782ac611"
   },
   "outputs": [],
   "source": [
    "gt = get_heads_gt(y, [elem.shape for elem in pred[0]], [2**i for i in range(3, 7+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59328d46"
   },
   "source": [
    "Let's visualize the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50d3e1ff"
   },
   "outputs": [],
   "source": [
    "def get_img(x):\n",
    "    return x.numpy().transpose(1,2,0)*stds + means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a970fef"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.tight_layout()\n",
    "n_scales = len(pred[0])\n",
    "plt.subplot(6,1+n_scales,1)\n",
    "plt.imshow(get_img(x[0]))\n",
    "for i in range(n_scales):\n",
    "    # classes\n",
    "    plt.subplot(6,1+n_scales, i+2)\n",
    "    plt.imshow(gt[0][i][0, 0].numpy())\n",
    "    # centerness\n",
    "    plt.subplot(6,1+n_scales, (1+n_scales) + i+2)\n",
    "    plt.imshow(gt[2][i][0, 0].numpy())\n",
    "    # regressions\n",
    "    for j in range(4):\n",
    "        plt.subplot(6,1+n_scales, (1+n_scales)*(2+j) + i+2)\n",
    "        plt.imshow(gt[1][i][0, j].numpy())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3dc4bdd"
   },
   "outputs": [],
   "source": [
    "def decode_boxes(offsets, positive_mask, stride):\n",
    "    ys, xs = torch.where(positive_mask)\n",
    "    xs = xs * stride + stride / 2\n",
    "    ys = ys * stride + stride / 2\n",
    "    offsets = offsets.permute(1,2,0)[positive_mask]\n",
    "    x_min = xs - offsets[:,0]\n",
    "    y_min = ys - offsets[:,1]\n",
    "    x_max = xs + offsets[:,2]\n",
    "    y_max = ys + offsets[:,3]\n",
    "    return torch.stack([x_min, y_min, x_max, y_max], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64af4806"
   },
   "outputs": [],
   "source": [
    "gt_cls, gt_reg, gt_centerness = gt\n",
    "\n",
    "print('gt_boxes:')\n",
    "print(y['xyxy'].numpy())\n",
    "for i in range(len(gt_cls)):\n",
    "    mask = gt_cls[i]>0\n",
    "    stride = 2**(i+3)\n",
    "    a = decode_boxes(gt_reg[i][0], mask[0,0], stride)\n",
    "    print(f'boxes from scale {i}')\n",
    "    print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "615fd629"
   },
   "source": [
    "Any ideas, what to do with multiple possible predictions on gt instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvpWfVSLnqpi"
   },
   "source": [
    "## Homework part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd5a3891"
   },
   "source": [
    "### 2.3 Losses (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMb2Iw7Mqyqo"
   },
   "source": [
    "In this part you need to fill the gaps in code for losses computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "654579c2"
   },
   "source": [
    "Classification loss is so called focal loss ([paper](https://arxiv.org/pdf/1708.02002.pdf)):\n",
    "\n",
    "$$\\text{FL}(p_t) = -(1-p_t)^\\gamma \\log p_t$$\n",
    "\n",
    "where $p_t = \\text{sigmoid}(x)$ for positive class and $p_t = 1- \\text{sigmoid}(x) = \\text{sigmoid}(-x)$ otherwise. \n",
    "\n",
    "For focal loss it's important to provide correct initialization of biases in the final layer that predicts logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ae3554"
   },
   "outputs": [],
   "source": [
    "gt_cls, gt_reg, gt_centerness = gt\n",
    "pred_cls, pred_reg, pred_centerness = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d53c4829"
   },
   "outputs": [],
   "source": [
    "def calc_focal_loss(pred, gt, alpha=0.25, gamma=2.):\n",
    "    \"\"\"\n",
    "    pred - tensor of shape [bs, n_classes, h, w]\n",
    "    gt - tensor of shape [bs, 1, h, w] with elements in [0... n_classes+1]\n",
    "    \"\"\"\n",
    "    bs, n_classes, h, w = pred.shape\n",
    "    one_hot = torch.zeros((bs, n_classes+1, h, w), dtype=torch.float32)\n",
    "    gt = gt.type(torch.int64)\n",
    "    one_hot.scatter_(dim=1, index=gt, src=torch.ones_like(gt, dtype=torch.float32))\n",
    "    one_hot = one_hot[:,1:]  # remove negative class\n",
    "\n",
    "    # YOUR CODE\n",
    "    p = ...\n",
    "    pt = ...\n",
    "    log_pt = ...\n",
    "\n",
    "    return torch.sum(-alpha * (1-pt)**gamma * log_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68fc4ae6"
   },
   "outputs": [],
   "source": [
    "for i in range(len(pred_cls)):\n",
    "    print(calc_focal_loss(pred_cls[i], gt_cls[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea050694"
   },
   "source": [
    "Loss for centerness is binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51a08d88"
   },
   "outputs": [],
   "source": [
    "def calc_centerness_loss(pred, gt):\n",
    "    return torch.sum(-gt*F.logsigmoid(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f6bf257"
   },
   "outputs": [],
   "source": [
    "for i in range(len(pred_cls)):\n",
    "    mask = gt_cls[i]>0\n",
    "    if torch.any(mask):\n",
    "        loss = calc_centerness_loss(pred_centerness[i][mask], gt_centerness[i][mask])\n",
    "    else:\n",
    "        loss = 0.0\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "883e58c7"
   },
   "source": [
    "Generalized IoU for regression ([paper](https://giou.stanford.edu/GIoU.pdf))\n",
    "<img src=\"https://miro.medium.com/max/1400/1*AO24IoYBsnneViLzEqrS0A.png\" style=\"width:40%\">\n",
    "\n",
    "Any ideas, why do we need object `C` here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcddf7e3"
   },
   "outputs": [],
   "source": [
    "def calc_giou_loss(x, y):\n",
    "    '''\n",
    "    x - [n_boxes, 4], xyxy format\n",
    "    y - [n_boxes, 4], xyxy format\n",
    "    '''\n",
    "    x_area = (x[:,2] - x[:,0]) * (x[:,3] - x[:,1])\n",
    "    y_area = (y[:,2] - y[:,0]) * (y[:,3] - y[:,1])\n",
    "\n",
    "    intersection_start = torch.maximum(x[:,:2], y[:,:2])\n",
    "    intersection_end = torch.minimum(x[:,2:], y[:,2:])\n",
    "    intersection_wh = (intersection_end - intersection_start).clip(min=0)\n",
    "    intersection_area = intersection_wh[:, 0] * intersection_wh[:,1]\n",
    "    \n",
    "    union_area = x_area + y_area - intersection_area\n",
    "    iou = intersection_area / union_area\n",
    "\n",
    "    # YOUR CODE: box for C\n",
    "    enclosing_box_start = ...\n",
    "    enclosing_box_end = ...\n",
    "    enclosing_box_wh = ...\n",
    "    enclosing_box_area = ...\n",
    "\n",
    "    giou_value = iou - (enclosing_box_area-union_area) / enclosing_box_area\n",
    "\n",
    "    giou_loss = 1. - giou_value\n",
    "    return torch.sum(giou_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d035fc4a"
   },
   "outputs": [],
   "source": [
    "for i in range(len(pred_cls)):\n",
    "    mask = gt_cls[i]>0\n",
    "    if torch.any(mask):\n",
    "        stride = 2**(i+3)\n",
    "        decoded_boxes_pred = decode_boxes(torch.exp(pred_reg[i][0]), mask[0,0], stride)\n",
    "        decoded_boxes_gt = decode_boxes(gt_reg[i][0], mask[0,0], stride)\n",
    "        loss = calc_giou_loss(decoded_boxes_pred, decoded_boxes_gt)\n",
    "    else:\n",
    "        loss = 0.0\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5461cc47"
   },
   "source": [
    "Combining all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab1ae3b9"
   },
   "outputs": [],
   "source": [
    "def compute_loss(pred, gt):\n",
    "    gt_cls, gt_reg, gt_centerness = gt\n",
    "    pred_cls, pred_reg, pred_centerness = pred\n",
    "\n",
    "    n_scales = len(pred)\n",
    "    cl_loss = 0.0\n",
    "    reg_loss = 0.0\n",
    "    cent_loss = 0.0\n",
    "    n_positive = 0\n",
    "    for i in range(n_scales):\n",
    "        cl_loss += calc_focal_loss(pred_cls[i].cpu(), gt_cls[i])\n",
    "        \n",
    "        mask = gt_cls[i]>0\n",
    "        n_positive += torch.sum(mask)\n",
    "        if torch.any(mask):\n",
    "            cent_loss += calc_centerness_loss(pred_centerness[i].cpu()[mask], gt_centerness[i][mask])\n",
    "        \n",
    "            stride = 2**(i+3)\n",
    "            decoded_boxes_pred = decode_boxes(torch.exp(pred_reg[i].cpu()[0]), mask[0,0], stride)\n",
    "            decoded_boxes_gt = decode_boxes(gt_reg[i][0], mask[0,0], stride)\n",
    "            reg_loss += calc_giou_loss(decoded_boxes_pred, decoded_boxes_gt)\n",
    "            \n",
    "    n_positive = n_positive.clip(20)\n",
    "    cl_loss = cl_loss / n_positive\n",
    "    reg_loss = reg_loss / n_positive\n",
    "    cent_loss = cent_loss / n_positive\n",
    "    total_loss = cl_loss + reg_loss + cent_loss\n",
    "    return total_loss, (cl_loss, reg_loss, cent_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd90a840"
   },
   "outputs": [],
   "source": [
    "compute_loss(pred, gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32b921fc"
   },
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Td3ZbGCoJgm"
   },
   "source": [
    "Now you need to train a model. The code for it is written, although you are welcome to change it as you wish to achieve better results.\n",
    "\n",
    "To get better results you may change the code above, if you want (including image preprocessing)\n",
    "\n",
    "Reference: if everything above is right, you should get validation loss <=0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "950189eb"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "detector = FCOS(n_classes=20)\n",
    "detector = detector.to(device)\n",
    "\n",
    "opt = torch.optim.Adam(detector.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcee6166"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "num_epochs = 100\n",
    "detector.train(False)   # disable batchnorms in resnet\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_comp_loss = []\n",
    "    val_comp_loss = []\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    \n",
    "    for batch_index, (X_batch, y_batch) in enumerate(tqdm.tqdm(train_batch_gen)):\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        pred = detector(X_batch)\n",
    "        gt = get_heads_gt(y_batch, [elem.shape for elem in pred[0]], [2**i for i in range(3, 7+1)])\n",
    "\n",
    "        loss, component_loss = compute_loss(pred, gt)\n",
    "        loss = loss / batch_size\n",
    "        loss.backward()\n",
    "        if batch_index % batch_size == 0:\n",
    "            # TODO: check if clipping is needed and adjust parameter\n",
    "            # clip_value = 1\n",
    "            # nn.utils.clip_grad_norm(detector.parameters(), clip_value)\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        train_loss.append(loss.data.numpy())\n",
    "        train_comp_loss.append(np.array([elem.data.numpy() for elem in component_loss]))\n",
    "        print(y_batch, component_loss)\n",
    "        if batch_index % 250 == 0:\n",
    "            print(\"current training loss: \\t{:.6f} , \\t component loss: {}\".format(\n",
    "                  np.mean(train_loss), np.mean(np.stack(train_comp_loss), axis=0)))\n",
    "        \n",
    "    for X_batch, y_batch in val_batch_gen:\n",
    "        X_batch = X_batch.to(device)\n",
    "\n",
    "        pred = detector(X_batch)\n",
    "        gt = get_heads_gt(y_batch, [elem.shape for elem in pred[0]], [2**i for i in range(3, 7+1)])\n",
    "\n",
    "        loss, component_loss = compute_loss(pred, gt)\n",
    "        val_loss.append(loss.data.numpy())\n",
    "        val_comp_loss.append(np.array([elem.data.numpy() for elem in component_loss]))\n",
    "        \n",
    "        if len(val_comp_loss) % 100 == 0:\n",
    "            print(\"current validation loss: \\t{:.2f} , \\t componnet loss: {}\".format(\n",
    "                  np.mean(val_loss), np.mean(np.stack(val_comp_loss), axis=0)))\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f} , \\t component loss: {}\".format(\n",
    "        np.mean(train_loss), np.mean(np.stack(train_comp_loss), axis=0)))\n",
    "    print(\"  validation loss: \\t\\t\\t{:.2f} , \\t\\t componnet loss: {}\".format(\n",
    "        np.mean(val_loss), np.mean(np.stack(val_comp_loss), axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a91718c"
   },
   "source": [
    "## 4. Inference stuff (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrsfKsc5oxGw"
   },
   "source": [
    "Here you'll need to complete the code for inference and metric computation.\n",
    "\n",
    "We'll use MAP for a metric. There's library `torchmetrics`  which provides a function to compute MAP (and other metrics). This library needs to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4b7b761d"
   },
   "outputs": [],
   "source": [
    "! pip install torchmetrics[detection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0Cb4dvLozJY"
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import nms\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlhKG_hSo0hX"
   },
   "outputs": [],
   "source": [
    "detector = torch.load(\"model_7_epoch.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsxEtB3Qo13H"
   },
   "source": [
    "#### Running inference on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NM7vZgoIo6GB"
   },
   "source": [
    "Your tasks here are:\n",
    "1. Add centerness scores to the computation of mask. \n",
    "\n",
    "Here's the line from the paper about how centerness is used:\n",
    "``\n",
    "When testing, the final score (used for ranking the detected bounding boxes) is computed by multiplying the predicted center-ness with the corresponding classification score. Thus the center-ness can downweight the scores of bounding boxes far from the center of an object. As a result, with high probability, these low-quality bounding boxes might be filtered out by the final non-maximum suppression (NMS) process, improving the detection performance remarkably.\n",
    "``\n",
    "\n",
    "2. Choose the hyperparameter thrs for mask computation. Actually this hyperparameter should be estimated using train and val sets, but here we won't ask to do that. Simply choose the value that suits you. Although you might write the code for proper parameter estimation =)\n",
    "3. Add NMS after computing bounding boxes. NMS in PyTorch: `torchvision.ops.nms()`, `torchvision.ops.batched_nms`. Choose the best iou_threshold parameter.\n",
    "4. Compute average precision metric for the test set and visualize detection results on some images from val data.\n",
    "\n",
    "To get the full points for this part you need to implement all the parts of the task (1-4) correctly. There are a couple of hints for you to understand that your code is correct:\n",
    "- correct implementation should achieve map score >= 0.1;\n",
    "- visually detection results should look acceptable. [Example](https://ibb.co/LPhHHsJ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBhvq49Ao3hJ"
   },
   "outputs": [],
   "source": [
    "n_images = 12\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.tight_layout()\n",
    "start = 0\n",
    "\n",
    "for i, (x,y) in enumerate(val_batch_gen):\n",
    "    \n",
    "    x = x.to(device)\n",
    "    pred_cls, pred_reg, pred_centerness  = detector(x)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    decoded_boxes = ...\n",
    "\n",
    "    if i < n_images:\n",
    "        x = x.cpu()\n",
    "        plt.subplot(3, n_images // 3, i+1)\n",
    "        img = x[0]*stds.reshape(3,1,1) + means.reshape(3,1,1)\n",
    "        img = img*255\n",
    "        img = img.type(torch.uint8)\n",
    "        img = torchvision.utils.draw_bounding_boxes(img, decoded_boxes, width=2)\n",
    "        plt.imshow(img.numpy().transpose(1,2,0))\n",
    "        plt.xticks([]); plt.yticks([])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
