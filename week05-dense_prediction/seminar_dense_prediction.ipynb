{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "f12hlbeiplvt94g5r23l2o"
   },
   "source": [
    "# Seminar 4 - Dense Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1\n",
    "You want to implement operation `Conv2dWithStride(in_channels, out_channels, kernel_size, padding, stride)`. \n",
    "\n",
    "You have a library that has operation `Conv2dWithoutStride(in_channels, out_channels, kernel_size, padding)` and slicing operation. \n",
    "\n",
    "How to implement `Conv2dWithStride` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "Conv2dWithoutStride = nn.Conv2d\n",
    "\n",
    "\n",
    "class Conv2dWithStride(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.nn = Conv2dWithoutStride(...\n",
    "        self.stride = ... \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case1():\n",
    "    with torch.no_grad():\n",
    "        conv_gt = nn.Conv2d(in_channels=10, out_channels=1, kernel_size=5, padding=2, stride=2)\n",
    "        conv_custom = Conv2dWithStride(in_channels=10, out_channels=1, kernel_size=5, padding=2, stride=2)\n",
    "        conv_custom.nn.weight = conv_gt.weight\n",
    "        conv_custom.nn.bias = conv_gt.bias\n",
    "\n",
    "        for _ in range(10):\n",
    "            data = np.random.random((1, 10, 16, 16))\n",
    "            data = torch.FloatTensor(data)\n",
    "            \n",
    "            res1 = conv_gt(data).numpy()\n",
    "            res2 = conv_custom(data).numpy()\n",
    "            assert np.allclose(res1, res2, atol=1e-5), np.linalg.norm(res1 - res2)\n",
    "            \n",
    "test_case1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2\n",
    "\n",
    "You have a library that has an operation `Conv2dWithoutStride(in_channels, out_channels, kernel_size, padding, dilation)`. This library don't provide slicing operation over torch tensors, but you still have slicing operation in numpy. You want to create Sequential which will work like Conv2d(..., stride=2) + Conv2d(..., stride=1) using `Conv2dWithoutStride`. \n",
    "\n",
    "How to do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Case2GT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=10, out_channels=2, kernel_size=5, padding=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        return y\n",
    "    \n",
    "class Case2Custom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.conv1 = Conv2dWithoutStride(...)\n",
    "        self.conv2 = Conv2dWithoutStride(...)\n",
    "        self.stride = 2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case2():\n",
    "    with torch.no_grad():\n",
    "        gt = Case2GT()\n",
    "        custom = Case2Custom()\n",
    "        \n",
    "        custom.conv1.weight = gt.conv1.weight\n",
    "        custom.conv1.bias = gt.conv1.bias\n",
    "        custom.conv2.weight = gt.conv2.weight\n",
    "        custom.conv2.bias = gt.conv2.bias\n",
    "\n",
    "        for _ in range(10):\n",
    "            data = np.random.random((1, 10, 16, 16))\n",
    "            data = torch.FloatTensor(data)\n",
    "            \n",
    "            res1 = gt(data).numpy()\n",
    "            res2 = custom(data).numpy()\n",
    "            assert np.allclose(res1, res2, atol=1e-5), np.linalg.norm(res1 - res2)\n",
    "            \n",
    "test_case2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3\n",
    "\n",
    "Same as in Case 2, but now the initial setup is Conv2d(..., stride=2) + Conv2d(..., stride=1) + Conv2d(..., stride=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Case3GT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=10, out_channels=2, kernel_size=5, padding=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        y = self.conv3(y)\n",
    "        return y\n",
    "    \n",
    "class Case3Custom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.conv1 = Conv2dWithoutStride(...)\n",
    "        self.conv2 = Conv2dWithoutStride(...)\n",
    "        self.conv3 = Conv2dWithoutStride(...)\n",
    "        self.stride = 2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case3():\n",
    "    with torch.no_grad():\n",
    "        gt = Case3GT()\n",
    "        custom = Case3Custom()\n",
    "        \n",
    "        custom.conv1.weight = gt.conv1.weight\n",
    "        custom.conv1.bias = gt.conv1.bias\n",
    "        custom.conv2.weight = gt.conv2.weight\n",
    "        custom.conv2.bias = gt.conv2.bias\n",
    "        custom.conv3.weight = gt.conv3.weight\n",
    "        custom.conv3.bias = gt.conv3.bias\n",
    "\n",
    "        for _ in range(10):\n",
    "            data = np.random.random((1, 10, 16, 16))\n",
    "            data = torch.FloatTensor(data)\n",
    "            \n",
    "            res1 = gt(data).numpy()\n",
    "            res2 = custom(data).numpy()\n",
    "            assert np.allclose(res1, res2, atol=1e-5), np.linalg.norm(res1 - res2)\n",
    "            \n",
    "test_case3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 4\n",
    "\n",
    "Same as in Case 3, but now the initial setup is Conv2d(..., stride=2) + Conv2d(..., stride=1) + Conv2d(..., **stride=2**) + Conv2d(..., stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Case4GT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=10, out_channels=2, kernel_size=5, padding=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1, stride=2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=4, out_channels=5, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.conv4(y)\n",
    "        return y\n",
    "    \n",
    "class Case4Custom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.conv1 = Conv2dWithoutStride(...)\n",
    "        self.conv2 = Conv2dWithoutStride(...)\n",
    "        self.conv3 = Conv2dWithoutStride(...)\n",
    "        self.conv4 = Conv2dWithoutStride(...)\n",
    "        self.stride = ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x[:, :, : : self.stride, : : self.stride]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case4():\n",
    "    with torch.no_grad():\n",
    "        gt = Case4GT()\n",
    "        custom = Case4Custom()\n",
    "        \n",
    "        custom.conv1.weight = gt.conv1.weight\n",
    "        custom.conv1.bias = gt.conv1.bias\n",
    "        custom.conv2.weight = gt.conv2.weight\n",
    "        custom.conv2.bias = gt.conv2.bias\n",
    "        custom.conv3.weight = gt.conv3.weight\n",
    "        custom.conv3.bias = gt.conv3.bias\n",
    "        custom.conv4.weight = gt.conv4.weight\n",
    "        custom.conv4.bias = gt.conv4.bias\n",
    "\n",
    "        for _ in range(10):\n",
    "            data = np.random.random((1, 10, 16, 16))\n",
    "            data = torch.FloatTensor(data)\n",
    "            \n",
    "            res1 = gt(data).numpy()\n",
    "            res2 = custom(data).numpy()\n",
    "            assert np.allclose(res1, res2, atol=1e-5), np.linalg.norm(res1 - res2)\n",
    "            \n",
    "test_case4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 5\n",
    "\n",
    "Replace sequence `flatten + Linear` with equivalent `Conv2d + Flatten`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.reshape(input.size(0), -1)\n",
    "    \n",
    "class Case5GT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.linear = nn.Linear(in_features=10*16*16, out_features=20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.flatten(x)\n",
    "        y = self.linear(y)\n",
    "        return y\n",
    "    \n",
    "class Case4Custom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.conv1 = nn.Conv2d(...)\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case5():\n",
    "    with torch.no_grad():\n",
    "        gt = Case5GT()\n",
    "        custom = Case5Custom()\n",
    "        \n",
    "        custom.conv1.weight = torch.nn.Parameter(gt.linear.weight.reshape(20, 10, 16, 16))\n",
    "        custom.conv1.bias = gt.linear.bias\n",
    "\n",
    "        for _ in range(10):\n",
    "            data = np.random.random((1, 10, 16, 16))\n",
    "            data = torch.FloatTensor(data)\n",
    "            \n",
    "            res1 = gt(data).numpy()\n",
    "            res2 = custom(data).numpy()\n",
    "            assert np.allclose(res1, res2, atol=1e-5), np.linalg.norm(res1 - res2)\n",
    "            \n",
    "test_case5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seminar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "jf8ijqxa1dotxk58ts2tle"
   },
   "source": [
    "Your task is to convert image classification network into fully-convolutional network that predicts value for every patch on image.\n",
    "\n",
    "What we have:\n",
    "* network trained to predict whether the central pixel of patch of size 114x114 belong to class 'road'\n",
    "* image that we want to segment\n",
    "\n",
    "Let's firstly look on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "5nobci51sb3e33308ecnw5"
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.misc\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "24yiz07nfclvswa5gy2w9"
   },
   "outputs": [],
   "source": [
    "# if running in colab or datasphere, uncomment this:\n",
    "#os.system(\"wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/spring21/seminar05-dense_prediction/um_000081-pred.png -O um_000081-pred.png\")\n",
    "#os.system(\"wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/spring21/seminar05-dense_prediction/layer_wrappers.py -O layer_wrappers.py\")\n",
    "#os.system(\"wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/spring21/seminar05-dense_prediction/um_000081.png -O um_000081.png\")\n",
    "#os.system(\"wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/spring21/seminar05-dense_prediction/um_road_000081.png -O um_road_000081.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "q88rtltbzco2jdc3aw954k"
   },
   "outputs": [],
   "source": [
    "%pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "w05atkgdtwifnofswj3z7l"
   },
   "outputs": [],
   "source": [
    "os.system(\"wget https://www.dropbox.com/s/429eqmyi3dbhzrg/model2.npz?dl=0 -O model2.npz\")\n",
    "# alternative link: https://yadi.sk/d/I1kHDWZTnH4DAw (for manual downloading only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "xjxp981xwnn73x76cncace"
   },
   "outputs": [],
   "source": [
    "def read_image(img_name, gt_name=None):\n",
    "    IMG_HEIGHT = 256\n",
    "    im = skimage.io.imread(img_name)\n",
    "    im = skimage.transform.rescale(im, IMG_HEIGHT * 1. / im.shape[0], multichannel=True)\n",
    "    im = skimage.img_as_ubyte(im)\n",
    "    if gt_name is not None:\n",
    "        gt = (skimage.io.imread(gt_name)[:,:,-1]==255).astype(np.uint8)*255\n",
    "        gt = skimage.transform.rescale(gt, IMG_HEIGHT * 1. / gt.shape[0], order=0, preserve_range=True)\n",
    "        gt = (gt > 0).astype(np.uint8)\n",
    "        return im, gt\n",
    "    return im\n",
    "\n",
    "def make_blending(img, labels, alpha=0.5):\n",
    "    colors = np.array([[0,0,0], [0,255,0]], np.uint8)\n",
    "    return (img*alpha + colors[labels.astype(np.int32)]*(1. - alpha)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "wu9blsee3peo6n71hxdae"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(3,1,1)\n",
    "im, gt = read_image('./um_000081.png', './um_road_000081.png')\n",
    "plt.imshow(im)\n",
    "plt.title('Source image')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(3,1,2)\n",
    "plt.imshow(make_blending(im, gt))\n",
    "plt.title('Ground truth')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(3,1,3)\n",
    "pred = skimage.io.imread('./um_000081-pred.png')\n",
    "plt.imshow(pred)\n",
    "plt.title('Expected prediction')\n",
    "plt.xticks([]); plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "5v9tnb5bp3e3qp60eqlbi"
   },
   "source": [
    "Semantic image segmentation problem could be considered as a problem of prediction label for the central pixel in image patch of predefined size. It allows us to use a lot of NN archtectures specifically designed for image classification (thanks to ImageNet Competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "v4olryidiwja1xjymssvfi"
   },
   "outputs": [],
   "source": [
    "def get_valid_patches(img_shape, patch_size, central_points):\n",
    "    start = central_points - patch_size/2\n",
    "    end = start + patch_size\n",
    "    mask = np.logical_and(start >= 0, end < np.array(img_shape))\n",
    "    mask = np.all(mask, axis=-1)\n",
    "    return mask\n",
    "\n",
    "def extract_patches(img, mask, n_pos=64, n_neg=64, patch_size=100):\n",
    "    res = []\n",
    "    labels = []\n",
    "    pos = np.argwhere(mask > 0)\n",
    "    accepted_patches_mask = get_valid_patches(np.array(img.shape[:2]), patch_size, pos)\n",
    "    pos = pos[accepted_patches_mask]\n",
    "    np.random.shuffle(pos)\n",
    "    for i in range(n_pos):\n",
    "        start = pos[i] - patch_size//2\n",
    "        end = start + patch_size\n",
    "        res.append(img[start[0]:end[0], start[1]:end[1]])\n",
    "        labels.append(1)\n",
    "        \n",
    "    neg = np.argwhere(mask == 0)\n",
    "    accepted_patches_mask = get_valid_patches(np.array(img.shape[:2]), patch_size, neg)\n",
    "    neg = neg[accepted_patches_mask]\n",
    "    np.random.shuffle(neg)\n",
    "    for i in range(n_neg):\n",
    "        start = neg[i] - patch_size//2\n",
    "        end = start + patch_size\n",
    "        res.append(img[start[0]:end[0], start[1]:end[1]])\n",
    "        labels.append(0)\n",
    "    return np.array(res), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "31mwmydpk9l8jobxhaequc"
   },
   "outputs": [],
   "source": [
    "patches, labels = extract_patches(im, gt, 32,32, patch_size=114)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "qi8ittez6zlifl8obwipf"
   },
   "source": [
    "Road-centered patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qlwmbr2cf8sbhpwrb4z6l"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(patches[i])\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "wtltcply7ffugd01ntps"
   },
   "source": [
    "Non-road-centered patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "p00zrphwnmpc981zphkse"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "for i in range(1,6):\n",
    "    plt.subplot(1,5,i)\n",
    "    plt.imshow(patches[-i])\n",
    "    plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ipftiqh421g0tv3iikztah"
   },
   "source": [
    "Here is our pretrained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "suiewmn1kyr2om25kb28uf"
   },
   "outputs": [],
   "source": [
    "with np.load('./model2.npz', encoding='latin1', allow_pickle=True) as f:\n",
    "    weights = f['state'].tolist()  # getting np.array content; it's dict in fact, not list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2wxm4b36s1jjt10tppfn9c"
   },
   "outputs": [],
   "source": [
    "from layer_wrappers import *\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.reshape(input.size(0), -1)\n",
    "    \n",
    "def create_network(weights):\n",
    "    net = nn.Sequential()\n",
    "    net.add_module('conv1_1', Conv2d(in_channels=3, out_channels=32, kernel_size=3, bias=False, \n",
    "                                     padding=0, weight_init=weights['conv1_1_w']))\n",
    "    net.add_module('bn1_1', BatchNorm2d(num_features=32, weight_init=weights['bn1_1_w'], \n",
    "                                        bias_init=weights['bn1_1_b'], mean_init=weights['bn1_1_mean'],\n",
    "                                        var_init=weights['bn1_1_var']))\n",
    "    net.add_module('relu1_1', nn.ReLU(inplace=True))\n",
    "    net.add_module('conv1_2', Conv2d(in_channels=32, out_channels=32, kernel_size=3, bias=False, padding=0, \n",
    "                                     weight_init=weights['conv1_2_w']))\n",
    "    net.add_module('bn1_2', BatchNorm2d(num_features=32, weight_init=weights['bn1_2_w'], \n",
    "                                        bias_init=weights['bn1_2_b'], mean_init=weights['bn1_2_mean'],\n",
    "                                        var_init=weights['bn1_2_var']))\n",
    "    net.add_module('relu1_2', nn.ReLU(inplace=True))\n",
    "    net.add_module('mp1', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
    "\n",
    "    net.add_module('conv2_1', Conv2d(in_channels=32, out_channels=64, kernel_size=3, \n",
    "                                     dilation=1, bias=False, padding=0, weight_init=weights['conv2_1_w']))\n",
    "    net.add_module('bn2_1', BatchNorm2d(num_features=64, weight_init=weights['bn2_1_w'], \n",
    "                                        bias_init=weights['bn2_1_b'], mean_init=weights['bn2_1_mean'],\n",
    "                                        var_init=weights['bn2_1_var']))\n",
    "    net.add_module('relu2_1', nn.ReLU(inplace=True))\n",
    "    net.add_module('conv2_2', Conv2d(in_channels=64, out_channels=64, kernel_size=3, \n",
    "                                     dilation=1, bias=False, padding=0, weight_init=weights['conv2_2_w']))\n",
    "    net.add_module('bn2_2', BatchNorm2d(num_features=64, weight_init=weights['bn2_2_w'], \n",
    "                                        bias_init=weights['bn2_2_b'], mean_init=weights['bn2_2_mean'],\n",
    "                                        var_init=weights['bn2_2_var']))\n",
    "    net.add_module('relu2_2', nn.ReLU(inplace=True))\n",
    "    net.add_module('mp2', nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, padding=0))\n",
    "\n",
    "    net.add_module('conv3_1', Conv2d(in_channels=64, out_channels=128, kernel_size=3, \n",
    "                                        dilation=1, bias=False, padding=0, weight_init=weights['conv3_1_w']))\n",
    "    net.add_module('bn3_1', BatchNorm2d(num_features=128, weight_init=weights['bn3_1_w'], \n",
    "                                        bias_init=weights['bn3_1_b'], mean_init=weights['bn3_1_mean'],\n",
    "                                        var_init=weights['bn3_1_var']))\n",
    "    net.add_module('relu3_1', nn.ReLU(inplace=True))\n",
    "    net.add_module('conv3_2', Conv2d(in_channels=128, out_channels=128, kernel_size=3, \n",
    "                                        dilation=1, bias=False, padding=0, weight_init=weights['conv3_2_w']))\n",
    "    net.add_module('bn3_2', BatchNorm2d(num_features=128, weight_init=weights['bn3_2_w'], \n",
    "                                           bias_init=weights['bn3_2_b'], mean_init=weights['bn3_2_mean'],\n",
    "                                        var_init=weights['bn3_2_var']))\n",
    "    net.add_module('relu3_2', nn.ReLU(inplace=True))\n",
    "    net.add_module('mp3', nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, padding=0))\n",
    "    \n",
    "    # 'mp3' output has shape [batch_size,128, 9, 9]\n",
    "    net.add_module('flatten', Flatten())\n",
    "    net.add_module('fc1', Linear(in_features=128*9*9, out_features=512, bias=False, weight_init=weights['fc1_w']))\n",
    "    net.add_module('fc1_bn', BatchNorm1d(num_features=512, weight_init=weights['fc1_bn_w'], \n",
    "                                         bias_init=weights['fc1_bn_b'], mean_init=weights['fc1_bn_mean'],\n",
    "                                        var_init=weights['fc1_bn_var']))\n",
    "    net.add_module('fc1_relu', nn.ReLU(inplace=True))\n",
    "    net.add_module('fc2', Linear(in_features=512, out_features=1, bias=True, \n",
    "                                 weight_init=weights['fc2_w'], bias_init=weights['fc2_b']))\n",
    "    net.add_module('probs', nn.Sigmoid())\n",
    "    \n",
    "    net = net.train(False)\n",
    "    return net\n",
    "\n",
    "def preproces(patches):\n",
    "    patches = patches.astype(np.float32)\n",
    "    patches = patches / 255 - 0.5\n",
    "    patches = patches.transpose(0,3,1,2)\n",
    "    return patches\n",
    "\n",
    "def apply_net(input_data, net):\n",
    "    input_data = Variable(torch.FloatTensor(input_data))\n",
    "    output = net(input_data).data.numpy()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "5plulfz2pvey0d17qdedzi"
   },
   "outputs": [],
   "source": [
    "net = create_network(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0tim37kffxyhpuk4e0gwy5"
   },
   "outputs": [],
   "source": [
    "predictions = apply_net(preproces(patches), net)\n",
    "predictions = (predictions > 0.5).ravel().astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "r8wa7goxv5z2kl8yi288d"
   },
   "outputs": [],
   "source": [
    "print(\"predictions: {}\".format(predictions))\n",
    "print(\"Accuracy: {}\".format((predictions == labels).mean()))\n",
    "print(\"Road class accuracy: {}; Non-road class accuracy: {}\".format(np.mean(predictions[:32] == 1), \n",
    "                                                          np.mean(predictions[32:] == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7ddgcadp0nbo8hfgvvfdic"
   },
   "source": [
    "### Your turn!\n",
    "\n",
    "Your task is to modify the network above to make it able to take image of arbitrary size as input and produce output of the same shape.\n",
    "\n",
    "Main changes:\n",
    "* Convert Linear layer to Conv2d (Don't forget to reshape weights from [n_out_features, n_in_features] to [n_out_filters, n_in_filters, kern_size, kern_sizse])\n",
    "* Replace BatchNorm1d with BatchNorm2d\n",
    "* Remove Flatten module\n",
    "* Remove strides from layers, add dilation in MaxPool2d and Conv2d (where it is needed)\n",
    "\n",
    "Known troubles:\n",
    "* MaxPool2d wants padding value to be less then kernel_size/2. If you need bigger value (and you will!), add nn.ConstantPad2d(padding_size, 0) before MaxPool2d (and don't forget to set padding=0 in MaxPool2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "npbkgn7eqyr6c2xs766yf"
   },
   "outputs": [],
   "source": [
    "from layer_wrappers import *\n",
    "\n",
    "def create_fully_conv_network(weights):\n",
    "    net = nn.Sequential()\n",
    "    # TODO\n",
    "    # it's better to start with copy-paste of 'create_network' function\n",
    "    net.add_module('probs', nn.Sigmoid())\n",
    "    net = net.train(False)\n",
    "    return net\n",
    "\n",
    "def preproces(patches):\n",
    "    patches = patches.astype(np.float32)\n",
    "    patches = patches / 255 - 0.5\n",
    "    patches = patches.transpose(0,3,1,2)\n",
    "    return patches\n",
    "\n",
    "def apply_net(input_data, net):\n",
    "    input_data = Variable(torch.FloatTensor(input_data))\n",
    "    output = net(input_data).data.numpy()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "wz1kgfboq6jiyk7kuzu9p"
   },
   "outputs": [],
   "source": [
    "net2 = create_fully_conv_network(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "z4qy2k7kibc2o1wh9d9wxt"
   },
   "outputs": [],
   "source": [
    "predictions = apply_net(preproces(patches[:5]), net2)\n",
    "assert predictions.shape[-2:] == patches.shape[1:3], \"{}, {}\".format(predictions.shape, patches[:5].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "d1rjcg4z5wo45t4m3djq9"
   },
   "source": [
    "Let's visualize what we finally have got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "84l2kmvrn274ajee2xxvjc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "patch_index = 1\n",
    "plt.subplot(1,4,1)\n",
    "plt.title('img')\n",
    "plt.imshow(patches[patch_index])\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(1,4,2)\n",
    "plt.title('pred')\n",
    "plt.imshow(predictions[patch_index,0])\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(1,4,3)\n",
    "plt.title('pred labels')\n",
    "plt.imshow(predictions[patch_index,0] > 0.5, 'gray')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(1,4,4)\n",
    "plt.title('blending')\n",
    "plt.imshow(make_blending(patches[patch_index], predictions[patch_index,0] > 0.5))\n",
    "plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "gcozk12aiyry5u3tuq58y"
   },
   "source": [
    "If everything is fine, you should be able to predict output for input image of any shape. Try crop 256x256 (or smaller if it doesn't fit in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "4pti55lmm9nyuwd3xwmoej"
   },
   "outputs": [],
   "source": [
    "plt.imshow(im)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9w9oqb23y8t8l8bhyvaprr"
   },
   "outputs": [],
   "source": [
    "patch = im[:, 200:200+256]\n",
    "predictions = apply_net(preproces(patch[np.newaxis]), net2)\n",
    "predictions.shape, patch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "5a5wm4hrm7rgqv02xykw6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,4,1)\n",
    "plt.title('img')\n",
    "plt.imshow(patch)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(1,4,2)\n",
    "plt.title('pred')\n",
    "plt.imshow(predictions[0,0])\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(1,4,3)\n",
    "plt.title('pred labels')\n",
    "plt.imshow(predictions[0,0] > 0.5, 'gray')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.subplot(1,4,4)\n",
    "plt.title('blending')\n",
    "plt.imshow(make_blending(patch, predictions[0,0] > 0.5))\n",
    "plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "kx2gwacyubxdr6ldb5df"
   },
   "outputs": [],
   "source": [
    "def split_and_apply(net, image, patch_w=150, overlap=80):\n",
    "    n_patches = image.shape[1] // patch_w\n",
    "    labels = np.zeros(image.shape[:2], np.uint8)\n",
    "    for i in range(n_patches):\n",
    "        print(i,n_patches)\n",
    "        patch = image[:, max(0, i*patch_w-overlap): min((i+1)*patch_w+overlap, image.shape[1])]\n",
    "        extra_left = i*patch_w - max(0, i*patch_w-overlap)\n",
    "        extra_right = min(image.shape[1], (i+1)*patch_w+overlap) - (i+1)*patch_w\n",
    "        out = (apply_net(preproces(patch[np.newaxis]), net)[0,0] > 0.5).astype(np.uint8)\n",
    "        labels[:, i*patch_w: (i+1)*patch_w] = out[:,extra_left:-extra_right]\n",
    "    if n_patches*patch_w < image.shape[1]:\n",
    "        last_patch_size = image.shape[1] - n_patches*patch_w\n",
    "        patch = image[:,-patch_w:]\n",
    "        labels[:,-last_patch_size:] = (apply_net(preproces(patch[np.newaxis]), net)[0,0] > 0.5).astype(np.uint8)[:,-last_patch_size:]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "xdwf9ggub59tyjl18bp8e"
   },
   "outputs": [],
   "source": [
    "#!M\n",
    "labels = split_and_apply(net2, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "c1smg0bkvd50o9syp5wbhff"
   },
   "outputs": [],
   "source": [
    "plt.imshow(make_blending(im, labels))\n",
    "plt.xticks([]); plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "y376brtwdwnsieudgzj7y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "27tgjcdaitw7em8dg13jic"
   },
   "source": [
    "### TL;DR DeepLab v2 architecture:\n",
    "![alt text](https://miro.medium.com/max/1283/1*8Lg66z7e7ijuLmSkOzhYvA.png \"Somthing wrong with image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "h3750jt5foe0v684hs3rcda"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "notebookId": "ed86a541-3574-4435-81cf-6cf077f60be3"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
