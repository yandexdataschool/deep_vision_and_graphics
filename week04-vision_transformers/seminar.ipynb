{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7872eeb5",
   "metadata": {},
   "source": [
    "# Vision Transformers 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3942fc",
   "metadata": {},
   "source": [
    "<img src=\"http://www.quickmeme.com/img/b6/b6131dc7b16f189fc83bece78c117e3df90fa1e1d8c5a1e7450e4d362c01181e.jpg\" style=\"width:40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d6823",
   "metadata": {},
   "source": [
    "Transformer is a family of neural network architectures that came to computer vision from NLP. Since transformers don't assume that their input has any specific structure, they can learn more general dependencies in data than convolutional neural network architectures. That's why we all like Vision transformers. At the same time, vision transformes are known to be \"data hungry\" and their training is quite tricky.\n",
    "\n",
    "Todays we will go through main components of vision transformers and their training procedure. This information will be quite useful for solving the next homework where your task will be to train vision transformer on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d5684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb93c2",
   "metadata": {},
   "source": [
    "## How to code your transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757cf50",
   "metadata": {},
   "source": [
    "<img src=\"./vit.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8ab73",
   "metadata": {},
   "source": [
    "As it was said, vision transformer came from NLP area where typical neural network input is ordered sequence of tokens which are words or word parts. So vision transformer main blocks are:\n",
    "1. Tokenizer - module that takes images and returns a set of tokens\n",
    "2. Transformer encoder - the main block of neural network that contains multihead attention, normalization and MLP on tokens.\n",
    "3. Positional embeddings - a way how to provide information about token orders\n",
    "4. Classification token - special token whose features is expected to be used for the final class prediction\n",
    "5. Classification head - MLP that predicts the final class from classificaiton token features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba2f4c",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799c007",
   "metadata": {},
   "source": [
    "Tokenizer should take an image, split it on non-overlapping patches, flatten the patches and apply Linear layer to these vectors. There are many ways how one can implement this, we will do it using Conv2D with stride being equal to kernel_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50abdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self, input_height, input_width, output_height, output_width,\n",
    "                 n_input_channels,\n",
    "                 embedding_dim):\n",
    "        super(Tokenizer, self).__init__()\n",
    "\n",
    "        assert input_height % output_height == 0, f\"{input_height} should be devided by {output_height}\"\n",
    "        assert input_width % output_width == 0, f\"{input_width} should be devided by {output_width}\"\n",
    "        \n",
    "        kernel_size = input_height // output_height\n",
    "        assert kernel_size == input_width // output_width\n",
    "\n",
    "        # YOUR CODE\n",
    "        self.conv = nn.Conv2d(...)\n",
    "\n",
    "        self.flattener = nn.Flatten(2, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.flattener(self.conv(x)).transpose(-2, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf6e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(input_height=32, input_width=32, output_height=16, output_width=16, n_input_channels=1,\n",
    "                      embedding_dim=64)\n",
    "dummy_batch = torch.zeros((1, 1, 32, 32))\n",
    "tokenizer_result = tokenizer.forward(dummy_batch)\n",
    "assert tokenizer_result.shape[1] == 16*16, tokenizer_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b7e5e",
   "metadata": {},
   "source": [
    "### Transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a00f51b",
   "metadata": {},
   "source": [
    "<img src=\"./transformer_encoder.png\" style=\"width:20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d258b",
   "metadata": {},
   "source": [
    "Transformer encoder consists of 2 blocks: Multi-Head Attention and MLP, each of each is prepended by layer norm. Let's walk through the separate modules for beggining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aee083",
   "metadata": {},
   "source": [
    "#### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b99e7f",
   "metadata": {},
   "source": [
    "<img src=\"https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eacf00",
   "metadata": {},
   "source": [
    "Attention implements a simple formula: $\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$.\n",
    "\n",
    "Multi-head attention is about splitting Q, K, V on several subvectors, appling Attention on each subvector independendly and concating the result.\n",
    "\n",
    "You can find Multi-Head Attention being implemented in pytorch as `torch.nn.MultiheadAttention`. Check the documentation and pay attention on `dropout` and `batch_first` parameters.\n",
    "\n",
    "[[paper]](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5327af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MultiheadAttention??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b57604",
   "metadata": {},
   "source": [
    "#### MLP for Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd93b81",
   "metadata": {},
   "source": [
    "MLP for transformer encoder is just a simple two-layer perceptron with GELU as non-linearity. It also uses Dropout after each Linear layer in order to reduce overfitting. Important thing is that size of hidden state on MLP is usually several times bigger than size of MLP input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845ef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(embedding_dim, mlp_size, dropout_rate):\n",
    "    return nn.Sequential(\n",
    "        # YOUR CODE: Linear + GELU + Dropout + Linear + Dropout\n",
    "        nn.Linear(..\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b94014c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = create_mlp(128, 128 * 2, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1056fdd",
   "metadata": {},
   "source": [
    "#### GELU\n",
    "\n",
    "Hey, wait! What is GELU? One can say that GELU is sort of smooth version of ReLU. Take a look"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb4cec",
   "metadata": {},
   "source": [
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.48.44_PM.png\" style=\"width:40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaed85e",
   "metadata": {},
   "source": [
    "More formally, GELU can be defined as $$GELU(x) = x \\cdot P(X \\le x) = x \\cdot \\Phi(x) = x \\cdot 12 (1 + \\text{erf}(\\frac{x}{\\sqrt{2}}))$$\n",
    "where $\\Phi(x)$ is the standard Gaussian cumulative distribution function.\n",
    "\n",
    "GELU can also be approximated as $$ GELU(x) = 0.5x(1 + \\tanh(\\sqrt{\\frac{2}{\\pi}} (x+ 0.044715x^3)))$$\n",
    "or $$GELU(x) = x \\sigma(1.702x)$$\n",
    "\n",
    "[[paper]](https://arxiv.org/abs/1606.08415v4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07328ab",
   "metadata": {},
   "source": [
    "#### Layer norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970438d",
   "metadata": {},
   "source": [
    "While Batch Normalization is a default normalization layer for convolutional neural networks, in transformers Layer Normalization is used instead. Moreover, Layer Norm is used in NLP-like manner:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c36740",
   "metadata": {},
   "source": [
    "<img src=\"./ln_in_vit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766c262",
   "metadata": {},
   "source": [
    "Image above is taken from [paper](https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Yao_Leveraging_Batch_Normalization_for_Vision_Transformers_ICCVW_2021_paper.pdf), where the authors checked whether it is possible to replace layer norm with good old batch norm (spoiler: you don't want to do that).\n",
    "\n",
    "Layer norm is implemented in pytorch as `torch.nn.LayerNorm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90723060",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LayerNorm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d00ac",
   "metadata": {},
   "source": [
    "#### Stochastic depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae6b36",
   "metadata": {},
   "source": [
    "Stochastic depth was introduced in [paper](https://arxiv.org/pdf/1603.09382.pdf) as a way of overfitting reduction. You can think of it as about dropout on residual branches. ResNet-block with stochastic depth module looks like `y = x + DropPath(ResidualNet(x))` instead of classic `y = x + ResidualNet(x)`\n",
    "\n",
    "Let's implement `DropPath` module. Its only parameter is `drop_prob` - probability to zero-out its input. Don't forget to devide the result on `(1-drop_prob)` in order to fix mean value of output in train mode (as you did in the first homework in dropout implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9394b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        # YOUR CODE: generate random tensor, binarize it, cast to x.dtype, multiply x by the mask, \n",
    "        # devide the result on keep_prob\n",
    "        random_tensor = torch.rand(...)\n",
    "        output = ...\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fdab4",
   "metadata": {},
   "source": [
    "#### TransformerEncoder: putting it all together\n",
    "\n",
    "Now we are ready to define Transformer Encoder.\n",
    "<img src=\"./transformer_encoder.png\" style=\"width:20%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ddf2fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_size, dropout=0.1, attention_dropout=0.1,\n",
    "                 drop_path_rate=0.1):\n",
    "        super().__init__()\n",
    "        # YOUR CODE\n",
    "        self.attention_pre_norm = ...\n",
    "        self.attention = torch.nn.MultiheadAttention(...)\n",
    "        self.attention_output_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mlp_pre_norm = ...\n",
    "        self.mlp = create_mlp(...)\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first block\n",
    "        y = self.attention_pre_norm(x)\n",
    "        attention = self.attention(y, y, y)[0]\n",
    "        attention = self.attention_output_dropout(attention)\n",
    "        x = x + self.drop_path(attention)   # Residual connection\n",
    "            \n",
    "        # second block\n",
    "        y = self.mlp_pre_norm(x)\n",
    "        y = self.mlp(y)\n",
    "        x = x + self.drop_path(y)  # Residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d4acf",
   "metadata": {},
   "source": [
    "Let's check that it actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "109eeb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87c8d2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "e = TransformerEncoder(embedding_dim=64, num_heads=2, mlp_size=128)\n",
    "encoder_result = e(tokenizer_result)\n",
    "print (encoder_result.shape)\n",
    "assert encoder_result.shape == tokenizer_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16e020",
   "metadata": {},
   "source": [
    "### Positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec405abd",
   "metadata": {},
   "source": [
    "Positional embeddings is a way to give transformer information about token orders. You can either learn good embeddings by SGD or use some scheme for embeddings generation. The most popular scheme is sinusoidal embeddings:\n",
    "\n",
    "$$\\text{emb}(p, 2i) = \\sin(\\frac{p}{10000^{2i/d}})$$\n",
    "$$\\text{emb}(p, 2i + 1) = \\cos(\\frac{p}{10000^{2i/d}})$$\n",
    "where p, 2i, 2i+1 - indices of embedding element, d - embedding dimension\n",
    "\n",
    "Tranditional way of using embeddings in pytorch is by `torch.nn.Embedding`. But in our case its simplier to use more low-level thing `torch.nn.Parameter`. Here is how one can define learnable embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a607f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 256\n",
    "embedding_dim = 64\n",
    "\n",
    "# YOUR CODE\n",
    "emb =  torch.nn.Parameter(...)\n",
    "\n",
    "_ = torch.nn.init.trunc_normal_(emb, std=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1553d647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2002, grad_fn=<StdBackward>) torch.Size([1, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "print(emb.std(), emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9879388",
   "metadata": {},
   "source": [
    "### Class token and classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53167f24",
   "metadata": {},
   "source": [
    "Vanilla Vision Transformer uses a rather unusual way how to get the embedding of the whole image for the final prediction. It adds one more token, named as class-token, with its own positional embedding and takes its features as the final embedding of image. Alternative approach that comes from CNN is to use global average pooling for image embeddings obtaining. While being more simple to implement, global average pooling introduces a shortcut how different patches can communicates between each other (in vanilla ViT all the inter-patch relations can be learned only through attention blocks).\n",
    "\n",
    "However in modern papers you can meet the both approaces equally likely.\n",
    "\n",
    "Adding class token in pytorch is simple thing. You can either add one more embedding to `nn.Parameter` for positional encoders or create one more `nn.Parameter` module for class token only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "356f572b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64]) torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "class_emb = torch.nn.Parameter(torch.empty((1, embedding_dim)), requires_grad=True)\n",
    "torch.nn.init.trunc_normal_(class_emb, std=0.2)\n",
    "\n",
    "print(class_emb[0].shape, class_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93121d04",
   "metadata": {},
   "source": [
    "### Vision Transformer: putting it all together\n",
    "\n",
    "<img src=\"./vit.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ccaab8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_height, input_width,\n",
    "                 n_tokens,\n",
    "                 n_input_channels,\n",
    "                 embedding_dim,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 num_classes=1000,\n",
    "                 mlp_ratio=4.0,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE\n",
    "        # 1. Tokenizer\n",
    "        self.tokenizer = Tokenizer(...)\n",
    "        \n",
    "        # 2. Positional embeddings\n",
    "        self.positional_embeddings = torch.nn.Parameter(...)\n",
    "        torch.nn.init.trunc_normal_(self.positional_embeddings, std=0.2)\n",
    "        \n",
    "        # 3. Class token\n",
    "        self.class_embedding = torch.nn.Parameter(torch.empty((1, embedding_dim)), requires_grad=True)\n",
    "        torch.nn.init.trunc_normal_(self.class_embedding, std=0.2)\n",
    "\n",
    "        # 4. TransformerEncoder with DropPath\n",
    "        mlp_size = int(embedding_dim * mlp_ratio)\n",
    "        \n",
    "        layers_drop_path_rate = [x.item() for x in torch.linspace(0, stochastic_depth, num_layers)]\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerEncoder(...)\n",
    "            for i in range(num_layers)])\n",
    "        \n",
    "        # 5. we will need more dropout and normalization!\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # 6. layer for the final prediction\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. tokenizer\n",
    "        patch_embeddings = ...\n",
    "        \n",
    "        # 2. position embeddings\n",
    "        patch_embeddings += self.positional_embeddings\n",
    "        \n",
    "        # 3. adding class token\n",
    "        cls_token = ...\n",
    "        \n",
    "        x = torch.cat(...)\n",
    "\n",
    "        # dropout!\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 4. transformer encoder blocks\n",
    "        for block in self.blocks:\n",
    "            x = ...\n",
    "            \n",
    "        # 5. final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # 6. final prediction from class-token embeddings\n",
    "        x = ...\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9adde091",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 16\n",
    "input_width = 16\n",
    "n_input_channels = 1\n",
    "vit = VisionTransformer(input_height, input_width,\n",
    "                 n_tokens=4,\n",
    "                 n_input_channels=n_input_channels,\n",
    "                 embedding_dim=32,\n",
    "                 num_layers=2,\n",
    "                 num_heads=2,\n",
    "                 num_classes=10,\n",
    "                 mlp_ratio=2.0,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f69ac628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 16])\n",
      "torch.Size([1, 10])\n",
      "tensor([[-0.1154, -0.4299, -0.3787, -0.0381, -0.4995,  0.4111, -0.5724,  0.2755,\n",
      "         -1.5983,  0.8585]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "fake_batch = torch.rand((1, n_input_channels, input_height, input_width))\n",
    "print(fake_batch.shape)\n",
    "\n",
    "result = vit(fake_batch)\n",
    "print(result.shape)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006b22b",
   "metadata": {},
   "source": [
    "## How to train your transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24e8a0",
   "metadata": {},
   "source": [
    "### Warm-up + scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3d885",
   "metadata": {},
   "source": [
    "You can train Vision Transformer using Adam or SGD, but you defenetely have to use scheduler that properly changes the learning rate. Good baseline is to use cosine learning rate that changes learning rate according to formula\n",
    "$$\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)$$\n",
    "where $\\eta_t$ - learning rate on epoch $t$, $\\eta_{min}$ minimum learning rate, $\\eta_{max}$ - initial learning rate, $T_{max}$ - maximum number of epochs.\n",
    "\n",
    "Here is example of how learning rate will change:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*BJCssPOCn4u__NoAZs392w.png\" style=\"width:70%\">\n",
    "\n",
    "[plot source](https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b)\n",
    "\n",
    "See this strange behaviour of scheduler at the begining? It's called warm-up. It's a linear increasing of learning rate from minimal to maximal value during the several initial (usually 5-10) epochs of training. Adding warm-up is important thing for transformer training.\n",
    "\n",
    "Check how schedulers can be implemented in torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d75470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.lr_scheduler.StepLR??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edc536",
   "metadata": {},
   "source": [
    "### Data augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e5f6d",
   "metadata": {},
   "source": [
    "As it was said, vision transformers are \"data hungry\". That's why one should use strong data augmentaiton techniques for training a good vision transformer.\n",
    "\n",
    "Most popular techniques are:\n",
    "  - Mixup\n",
    "  - Cutmix\n",
    "  - Cutout\n",
    "  - RandAugment\n",
    "  - AutoAugment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb89886",
   "metadata": {},
   "source": [
    "#### Cutout / MixUp / CutMix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89914b4",
   "metadata": {},
   "source": [
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-07_at_1.40.03_PM.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629343bd",
   "metadata": {},
   "source": [
    "[CutOut](https://arxiv.org/abs/1708.04552) exploits a very simple idea: just zero-out some (probably large) area of image in order to make network more robust to occlusions.\n",
    "\n",
    "[MixUp](https://arxiv.org/abs/1710.09412) is more tricky things. It suggest to sample pairs of images, create some linear interpolation of these images with random weights and train the network to predict weighted average of the labels of the original images. More formally, it assumes that network input is defined as\n",
    "$$x=\\lambda x_i + (1−\\lambda) x_j$$\n",
    ",where $x_i$, $x_j$ are raw input images.\n",
    "\n",
    "The network is trained to predict\n",
    "$$y = \\lambda y_i + (1−\\lambda) y_j$$\n",
    ", where $y_i$, $y_j$ are one-hot label encodings.\n",
    "\n",
    "Let's implement this augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7eecbf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "    for x,y in train_data:\n",
    "        mixed_x, y_a, y_b, mixup_coeff = mixup_data(x,y)\n",
    "        logits = model(mixed_x)\n",
    "        \n",
    "        loss1 = compute_loss(logits, y_a)\n",
    "        loss2 = compute_loss(logits, y_b)\n",
    "        loss = loss1 * mixup_coeff + (1. - mixup_coeff) * loss2\n",
    "        loss.backward()\n",
    "    \"\"\"\n",
    "    \n",
    "    if alpha > 0:\n",
    "        mixup_coeff = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        mixup_coeff = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "    \n",
    "    #YOUR CODE\n",
    "    mixed_x = ...\n",
    "\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, mixup_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02936d",
   "metadata": {},
   "source": [
    "Finally [CutMix](https://arxiv.org/pdf/1905.04899.pdf) is kind of a combination of mixup and cutout. It randomly selects a region of input image (like CutOut) and pastes a part of another image in this region. It makes input images less creapy than in MixUp and forces neural network to learn features that are more sensitive to local information.\n",
    "<img src=\"https://miro.medium.com/max/318/1*STI1O9RPfEC9hsPXCHWSbg.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32fae3b",
   "metadata": {},
   "source": [
    "#### RandAugment / AutoAugment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f703a",
   "metadata": {},
   "source": [
    "There is one more thing in augmentation created especially for lazy data scientists. Authors of [RandAugment](https://arxiv.org/pdf/1909.13719.pdf) suggested to collect all the powerful augmentations in one place and use only two parameter for tuning all of them simultaniously.\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/media/EJtAflaUUAAPLWv.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b517f",
   "metadata": {},
   "source": [
    "Here is an example how different magnitude affects the final image.\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_11.09.47_PM.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcaefac",
   "metadata": {},
   "source": [
    "The previous approach to build uber-augmentation tool on the basis of simple augmentation tools is called [AutoAugment](https://arxiv.org/abs/1805.09501). It is much more difficult approach comparing to RandAugment. In a nutshell it suggests to train RNN for sampling a better augmentation policy.\n",
    "<img src=\"https://user-images.githubusercontent.com/26705935/61943777-27ad2980-afd7-11e9-8a16-d6d4a7ac192a.png\" style=\"width:50%\">\n",
    "\n",
    "You still can find new papers which use AutoAugment in their experiments, but RandAugment is becoming a simple and effective replacement for AutoAugment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c0b07",
   "metadata": {},
   "source": [
    "### As a conclustion\n",
    "\n",
    "Today there were a lot of lecture-like material and a small amount of real practice. But don't be upset. Your next homework will be about training ViT on CIFAR-10, so you will have opportunity to test all the stuff above. \n",
    "\n",
    "See you in homework 2-2!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae2358e",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Attention is all you need](https://arxiv.org/abs/1706.03762) - original paper that introduces transformer architecture for NLP tasks.\n",
    "* [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) - paper that introduces Vision Transformers\n",
    "* [How to train your ViT? Data, Augmentation,and Regularization in Vision Transformers](https://arxiv.org/pdf/2106.10270.pdf) - experiments on the importance of data augmentation and regularization\n",
    "* [ViT on GitHub](https://github.com/google-research/vision_transformer) - original implementation of ViT\n",
    "* [ViT on GitHub2](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py) - good reimplementation in pytorch (`timm` library)\n",
    "* [Improved Regularization of Convolutional Neural Networks with Cutout](https://arxiv.org/abs/1708.04552) - cutout \n",
    "* [mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412) - mixup\n",
    "* [CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features](https://arxiv.org/pdf/1905.04899.pdf) - CutMix\n",
    "* [RandAugment: Practical automated data augmentationwith a reduced search space](https://arxiv.org/pdf/1909.13719.pdf) - RandAugment\n",
    "* [SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983v5) - cosine annealing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2815dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
